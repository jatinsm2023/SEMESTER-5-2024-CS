{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f028ab35-da15-43eb-aa5f-746bf56d354f",
   "metadata": {},
   "source": [
    "### Logistic Regression Task\n",
    "\n",
    "#### Objective:\n",
    "Perform binary classification of a dataset to predict the 10-year risk of coronary heart disease (CHD) into two classes:\n",
    "- Having risk (1)\n",
    "- Not having risk (0)\n",
    "\n",
    "#### Dataset Details:\n",
    "\n",
    "**1. Demographic:**\n",
    "- **Sex:** Male or Female\n",
    "- **Age:** Age of the patient\n",
    "- **Education:** No further information provided\n",
    "\n",
    "**2. Behavioral:**\n",
    "- **Current Smoker:** Whether or not the patient is a current smoker\n",
    "- **Cigs Per Day:** The number of cigarettes that the person smoked on average per day\n",
    "\n",
    "**3. Information on Medical History:**\n",
    "- **BP Meds:** Whether or not the patient was on blood pressure medication\n",
    "- **Prevalent Stroke:** Whether or not the patient had previously had a stroke\n",
    "- **Prevalent Hyp:** Whether or not the patient was hypertensive\n",
    "- **Diabetes:** Whether or not the patient had diabetes\n",
    "\n",
    "**4. Information on Current Medical Condition:**\n",
    "- **Tot Chol:** Total cholesterol level\n",
    "- **Sys BP:** Systolic blood pressure\n",
    "- **Dia BP:** Diastolic blood pressure\n",
    "- **BMI:** Body Mass Index\n",
    "- **Heart Rate:** Heart rate (considered continuous in medical research)\n",
    "- **Glucose:** Glucose level\n",
    "\n",
    "**Target Variable:**\n",
    "- **10-year Risk of Coronary Heart Disease (CHD):** Binary classification (1 = Yes, 0 = No)\n",
    "\n",
    "#### Tasks:\n",
    "1. Use Logistic Regression to classify the risk of coronary heart disease based on the provided attributes.\n",
    "2. Evaluate the model's accuracy using appropriate evaluation metrics.\n",
    "eans “Yes”, “0” means “No”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "de3e8133-9d2a-45b7-857e-ab47c0dbd3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e94e64-2b86-46f9-bcfe-887b636edd4c",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "We read the csv file into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b689b74d-e8c3-4049-98ab-205b3840f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'framingham.csv'\n",
    "\n",
    "df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3a0e83-160e-4911-9458-ef2403f9574d",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "We explore the first few rows of the dataset and describe it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "db107550-e1a2-4d12-a30d-0d48c684117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
      "0     1   39        4.0              0         0.0     0.0                0   \n",
      "1     0   46        2.0              0         0.0     0.0                0   \n",
      "2     1   48        1.0              1        20.0     0.0                0   \n",
      "3     0   61        3.0              1        30.0     0.0                0   \n",
      "4     0   46        3.0              1        23.0     0.0                0   \n",
      "\n",
      "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
      "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
      "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
      "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
      "3             1         0    225.0  150.0   95.0  28.58       65.0    103.0   \n",
      "4             0         0    285.0  130.0   84.0  23.10       85.0     85.0   \n",
      "\n",
      "   TenYearCHD  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           1  \n",
      "4           0  \n",
      "\n",
      "Summary statistics:\n",
      "              male          age    education  currentSmoker   cigsPerDay  \\\n",
      "count  4240.000000  4240.000000  4135.000000    4240.000000  4211.000000   \n",
      "mean      0.429245    49.580189     1.979444       0.494104     9.005937   \n",
      "std       0.495027     8.572942     1.019791       0.500024    11.922462   \n",
      "min       0.000000    32.000000     1.000000       0.000000     0.000000   \n",
      "25%       0.000000    42.000000     1.000000       0.000000     0.000000   \n",
      "50%       0.000000    49.000000     2.000000       0.000000     0.000000   \n",
      "75%       1.000000    56.000000     3.000000       1.000000    20.000000   \n",
      "max       1.000000    70.000000     4.000000       1.000000    70.000000   \n",
      "\n",
      "            BPMeds  prevalentStroke  prevalentHyp     diabetes      totChol  \\\n",
      "count  4187.000000      4240.000000   4240.000000  4240.000000  4190.000000   \n",
      "mean      0.029615         0.005896      0.310613     0.025708   236.699523   \n",
      "std       0.169544         0.076569      0.462799     0.158280    44.591284   \n",
      "min       0.000000         0.000000      0.000000     0.000000   107.000000   \n",
      "25%       0.000000         0.000000      0.000000     0.000000   206.000000   \n",
      "50%       0.000000         0.000000      0.000000     0.000000   234.000000   \n",
      "75%       0.000000         0.000000      1.000000     0.000000   263.000000   \n",
      "max       1.000000         1.000000      1.000000     1.000000   696.000000   \n",
      "\n",
      "             sysBP        diaBP          BMI    heartRate      glucose  \\\n",
      "count  4240.000000  4240.000000  4221.000000  4239.000000  3852.000000   \n",
      "mean    132.354599    82.897759    25.800801    75.878981    81.963655   \n",
      "std      22.033300    11.910394     4.079840    12.025348    23.954335   \n",
      "min      83.500000    48.000000    15.540000    44.000000    40.000000   \n",
      "25%     117.000000    75.000000    23.070000    68.000000    71.000000   \n",
      "50%     128.000000    82.000000    25.400000    75.000000    78.000000   \n",
      "75%     144.000000    90.000000    28.040000    83.000000    87.000000   \n",
      "max     295.000000   142.500000    56.800000   143.000000   394.000000   \n",
      "\n",
      "        TenYearCHD  \n",
      "count  4240.000000  \n",
      "mean      0.151887  \n",
      "std       0.358953  \n",
      "min       0.000000  \n",
      "25%       0.000000  \n",
      "50%       0.000000  \n",
      "75%       0.000000  \n",
      "max       1.000000  \n",
      "\n",
      "Data types of each column:\n",
      "male                 int64\n",
      "age                  int64\n",
      "education          float64\n",
      "currentSmoker        int64\n",
      "cigsPerDay         float64\n",
      "BPMeds             float64\n",
      "prevalentStroke      int64\n",
      "prevalentHyp         int64\n",
      "diabetes             int64\n",
      "totChol            float64\n",
      "sysBP              float64\n",
      "diaBP              float64\n",
      "BMI                float64\n",
      "heartRate          float64\n",
      "glucose            float64\n",
      "TenYearCHD           int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nData types of each column:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857a5a06-388c-452e-971c-62131ae4e9c4",
   "metadata": {},
   "source": [
    "### Handling missing data\n",
    "1. Option 1: Fill missing values with a specific value (e.g., mean)\n",
    "2. Option 2: Drop rows/columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "920d80c8-f0dd-4e3d-bc43-8b2efff2e052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "male                 0\n",
      "age                  0\n",
      "education          105\n",
      "currentSmoker        0\n",
      "cigsPerDay          29\n",
      "BPMeds              53\n",
      "prevalentStroke      0\n",
      "prevalentHyp         0\n",
      "diabetes             0\n",
      "totChol             50\n",
      "sysBP                0\n",
      "diaBP                0\n",
      "BMI                 19\n",
      "heartRate            1\n",
      "glucose            388\n",
      "TenYearCHD           0\n",
      "dtype: int64\n",
      "\n",
      "Missing values were found and have been filled with the mean of each column.\n",
      "\n",
      "Missing values after handling:\n",
      "male               0\n",
      "age                0\n",
      "education          0\n",
      "currentSmoker      0\n",
      "cigsPerDay         0\n",
      "BPMeds             0\n",
      "prevalentStroke    0\n",
      "prevalentHyp       0\n",
      "diabetes           0\n",
      "totChol            0\n",
      "sysBP              0\n",
      "diaBP              0\n",
      "BMI                0\n",
      "heartRate          0\n",
      "glucose            0\n",
      "TenYearCHD         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "if missing_values.sum() > 0:\n",
    "    df.fillna(df.mean(), inplace=True)\n",
    "    print(\"\\nMissing values were found and have been filled with the mean of each column.\")\n",
    "\n",
    "    #df.dropna(inplace=True)\n",
    "    #print(\"\\nMissing values were found and have been dropped.\")\n",
    "    \n",
    "    print(\"\\nMissing values after handling:\")\n",
    "    print(df.isnull().sum())\n",
    "else:\n",
    "    print(\"\\nNo missing values found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489c9b69-8890-43e9-b951-8dbe49fea53f",
   "metadata": {},
   "source": [
    "### Defining Features and Targets\n",
    "We create 2 numpy arrays $X$ and $y$ from the dataframe:\n",
    "+ $X$: Input data of the shape (number of samples, number of input features)\n",
    "+ $y$: Target variable of the shape (number of samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4c5ad68e-4d4e-4cec-892e-747f0a05d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (4240, 15) , Shape of y:  (4240,)\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['TenYearCHD']).values\n",
    "y = df['TenYearCHD'].values\n",
    "\n",
    "print(\"Shape of X: \", X.shape, \", Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3d9f97-2574-4663-af35-4697003b078e",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "We pre-process the data using min-max scaler to scale the input data $X$.\n",
    "After that, we split the data into a training dataset (**X_train** and **y_train**) and test dataset (**X_test and y_test**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "854952a7-6194-4883-8aee-aba9bb48d351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (3180, 15) Shape of y_train:  (3180,)\n",
      "Shape of X_test:  (1060, 15) Shape of y_test:  (1060,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_index = int(X.shape[0] * (1 - test_size))\n",
    "\n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices = indices[split_index:]\n",
    "\n",
    "    X_train = X[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_train = y[train_indices]\n",
    "    y_test = y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def min_max_scaler(X, feature_range=(0, 1)):\n",
    "    X_min = np.min(X, axis=0)\n",
    "    X_max = np.max(X, axis=0)\n",
    "\n",
    "    X_scaled = (X-X_min)/(X_max-X_min)\n",
    "\n",
    "    return X_scaled\n",
    "\n",
    "X = min_max_scaler(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n",
    "print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb18f5d-b57a-4c9e-943e-604ad44986e8",
   "metadata": {},
   "source": [
    "### Creating the Sigmoid Function\n",
    "For logistic regression, the model is represented as\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
    "\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "\n",
    "The function below returns the value of the sigmoid function for an input numpy array z. If the numpy array 'z' stores multiple numbers, we'd like to apply the sigmoid function to each value in the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f870dc6c-593f-447e-8de4-47d2a9d8eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    z = z.astype(float)\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a39c1-d8f2-4ff6-9fea-196c90a02fc1",
   "metadata": {},
   "source": [
    "### Computing the loss Function\n",
    "\n",
    "For logistic regression, the cost function is of the form\n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is -\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
    "\n",
    "Here,\n",
    "\n",
    " **X**: input features\n",
    "\n",
    " **y**:  class labels\n",
    "\n",
    " **w**: Parameters of the logistic regression model, (excluding the bias), a numpy array of the shape(1, number of features)\n",
    "\n",
    " **b**:  Bias value of the logistic regression model\n",
    "\n",
    "We use the Sigmoid function implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a6c43239-96f2-463d-b5b5-1884867e340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "\n",
    "    z = np.dot(X, w) + b\n",
    "    fx = sigmoid(z)\n",
    "   \n",
    "    cost = (-y * np.log(fx) - (1 - y) * np.log(1 - fx))  \n",
    "    total_cost = np.sum(cost) / m\n",
    "\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba640be-4614-4456-b456-3bca199e7cff",
   "metadata": {},
   "source": [
    "### 2.4. Computing the Gradient of the Loss\n",
    "\n",
    "The gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$, $w_j$ are all updated simultaniously\n",
    "\n",
    "The `compute_gradient_logistic_regression` function computes $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "81b3238d-b709-4100-a0bd-dccc3dd93279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_regression(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.0\n",
    "\n",
    "    z = np.dot(X, w) + b\n",
    "    \n",
    "    fx = sigmoid(z)\n",
    "     \n",
    "    loss = fx - y\n",
    "\n",
    "    dj_dw = np.dot(loss.T, X).T / m\n",
    "    dj_db = np.sum(loss) / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6180a-9d68-4a2c-969d-1fe397585991",
   "metadata": {},
   "source": [
    "### Training the model using Batch Gradient Descent\n",
    "\n",
    "The batch gradient descent algorithm for logistic regression trains and learns the parameters of the logistic regression model. We use ```loss_function``` and ```compute_gradient_logistic regression``` functions that we implemented above.\n",
    "\n",
    "In this ```batch_gradient_descent_logistic_regression``` function, we compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n",
    "\n",
    "+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
    "\n",
    "+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "Additionally, we compute the loss function values in every iteration and store it in the list variable ```loss_hist``` and print the loss value after every 100 iterations during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "39113e64-7836-4226-9a77-84e22c4cef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent_logistic_regression(X, y, w_in, b_in, alpha, num_iters):\n",
    "    m = len(X)\n",
    "    loss_hist = []\n",
    "\n",
    "    for i in range(num_iters ):\n",
    "        dj_db, dj_dw = compute_gradient_logistic_regression(X, y, w_in, b_in)\n",
    "\n",
    "    \n",
    "        w_in -= alpha * dj_dw   \n",
    "        b_in -= alpha * dj_db\n",
    "        \n",
    "        loss = loss_function(X, y, w_in, b_in)\n",
    "        loss_hist.append(loss)\n",
    "\n",
    "        if (i % 100 == 0):\n",
    "            print(f\"Iteration: {i} Loss: {loss}\")\n",
    "    \n",
    "    return w_in, b_in, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b26a34-af7b-4758-bbff-c8807cc018b0",
   "metadata": {},
   "source": [
    "### Initialising parameters\n",
    "Now we intialize the model parameters ($w$ and $b$) and learning rate (**alpha**). The learning rate **alpha** is randomly initialized between 0.01 and 0.09. To randomly initialize the learning rate, we set a seed and use the random function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2bcb7a85-44c4-40e2-a30c-c5aa453b200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 32\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "def initialize_parameters():\n",
    "    initial_w = np.random.random(size=(X_train.shape[1], ))\n",
    "    initial_b = random.random()\n",
    "    alpha = random.uniform(0.01, 0.09)\n",
    "\n",
    "    return initial_w,initial_b,alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b53d9e-7a95-450f-8871-805a35b17fe7",
   "metadata": {},
   "source": [
    "### Running the batch gradient descent\n",
    "We run the batch gradient algorithm for num_iterations=1000 to train the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "44ff8562-6270-4881-80fd-4ac8d667b6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 2.453616970877461\n",
      "Iteration: 100 Loss: 0.4287638193580412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 200 Loss: 0.4205246542639541\n",
      "Iteration: 300 Loss: 0.4169178362116194\n",
      "Iteration: 400 Loss: 0.4141138099555775\n",
      "Iteration: 500 Loss: 0.4118529497521785\n",
      "Iteration: 600 Loss: 0.4099791363516742\n",
      "Iteration: 700 Loss: 0.40838756622194233\n",
      "Iteration: 800 Loss: 0.40700679894798353\n",
      "Iteration: 900 Loss: 0.40578737938788967\n",
      "optimized parameter values w: [ 0.38136001  0.18527652 -0.36302834 -0.12456765  0.53314462  0.73999178\n",
      "  0.17621518  0.72712813  0.64867325  0.13657894 -0.16756632 -0.38145459\n",
      "  0.0313858  -0.33064983  0.32465943]\n",
      "optimized parameter value b: -1.9236144372650754\n"
     ]
    }
   ],
   "source": [
    "initial_w, initial_b, alpha = initialize_parameters()\n",
    "num_iterations = 1000\n",
    "\n",
    "w, b, loss_hist = batch_gradient_descent_logistic_regression(X_train ,y_train, initial_w, initial_b, alpha, num_iterations)\n",
    "print(\"optimized parameter values w:\", w)\n",
    "print(\"optimized parameter value b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2183f0f-9f8e-448c-990e-f61628a10a5f",
   "metadata": {},
   "source": [
    "### Final Train and Test Accuracy\n",
    "After the logistic regression model is trained, we will predict the class labels for the training set and test set and we will compute the accuracy.\n",
    "\n",
    "- First we to compute the prediction from the model $f(x^{(i)}) = g(w \\cdot x^{(i)})$ for every example\n",
    "\n",
    "- We interpret the output of the model ($f(x^{(i)})$) as the probability that $y^{(i)}=1$ given $x^{(i)}$ and parameterized by $w$.\n",
    "- Therefore, to get a final prediction ($y^{(i)}=0$ or $y^{(i)}=1$) from the logistic regression model, we use the following heuristic -\n",
    "\n",
    "  if $f(x^{(i)}) >= 0.5$, predict $y^{(i)}=1$\n",
    "  \n",
    "  if $f(x^{(i)}) < 0.5$, predict $y^{(i)}=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "422c4d29-ad47-4129-8ce1-fbab3eb958f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b):\n",
    "      m, n = X.shape\n",
    "      p = np.zeros(m)\n",
    "    \n",
    "      z = np.dot(X,w)+b\n",
    "      y_predict = sigmoid(z)\n",
    "      \n",
    "      for i in range(m):\n",
    "          if y_predict[i]>=0.5:\n",
    "              p[i] = 1\n",
    "          else:\n",
    "              p[i]\n",
    "              \n",
    "      return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c252a6b-2a1e-485d-9ed7-6930cbe197ca",
   "metadata": {},
   "source": [
    "Now let's use this to compute the accuracy on the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8c6bd51f-d1e5-47c4-8333-bbfe9052998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 84.748428\n",
      "Test Accuracy: 85.094340\n"
     ]
    }
   ],
   "source": [
    "p_train = predict(X_train, w,b)\n",
    "print('Train Accuracy: %f'%(np.mean(p_train == y_train) * 100))\n",
    "p_test = predict(X_test, w,b)\n",
    "print('Test Accuracy: %f'%(np.mean(p_test == y_test) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c90cde",
   "metadata": {},
   "source": [
    "Now, we plot the loss function values for every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bbd1044c-9cae-4cc0-a1fc-2e84abe01a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_plot = [loss_hist[i] for i in range(len(loss_hist))]\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss function\")\n",
    "plt.plot(loss_plot)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
