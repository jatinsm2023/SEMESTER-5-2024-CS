{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Programming Assignment 2 Part 1 - Decision Tree"]},{"cell_type":"markdown","metadata":{},"source":["In this programming assignment, you will implement a decision tree model.\n","\n","*For Even Roll Number Students:*\n","\n","* In this part, you have to implement a decision tree model to predict the cardio vascular disease based on various input features.\n","* Noiseless Dataset: ````cardio.csv````\n","* Noisy Dataset: ````cardio_noise.csv````\n","\n","*For Odd Roll Number Students:*\n","\n","* In this part, you have to implement a decision tree model to predict whether a patient has diabetes based on various input features.\n","* Noiseless Dataset: ````diabetes.csv````\n","* Noisy Dataset: ````diabetes_noise.csv````\n","\n","The assignment zip file (ML_CS60050_A2.zip) contains the respective datasets which will be used in this assignment.\n","\n","You have to write your code in this jupyter notebook. You have to write your code only between ### START CODE HERE ### and ### END CODE HERE ### comments."]},{"cell_type":"markdown","metadata":{},"source":["### Assignment Submission Instructions\n","\n","Please submit your assignment as a ZIP file that contains a folder named in the following format: `RollNo_ML_A2`. Inside this folder, include two Jupyter notebooks and a Report with the following names:\n","\n","1. `RollNo_A2_Part1.ipynb`\n","2. `RollNo_A2_Part2.ipynb`\n","3. `RollNo_report.pdf`\n","\n","\n","Instructions for the Report:\n","* Summarize results from noiseless and noisy datasets.\n","* Compare performance and note the impact of noise.\n","* Conclude with key findings andÂ implications.\n","\n","Make sure that you replace `RollNo` with your actual roll number in both the folder name and the notebook filenames.\n","\n","For example, if your roll number is `23CS60R11`, the folder should be named `23CS60R11_ML_A2`, and the three files should be named `23CS60R11_A2_Part1.ipynb`, `23CS60R11_A2_Part2.ipynb` and `RollNo_report.pdf`.\n","\n","Submit this ZIP file as your assignment submission."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pandas in /home/jatin/.local/lib/python3.10/site-packages (2.2.2)\n","Requirement already satisfied: numpy in /home/jatin/.local/lib/python3.10/site-packages (2.0.1)\n","Requirement already satisfied: seaborn in /home/jatin/.local/lib/python3.10/site-packages (0.13.2)\n","Requirement already satisfied: matplotlib in /home/jatin/.local/lib/python3.10/site-packages (3.9.2)\n","Requirement already satisfied: tzdata>=2022.7 in /home/jatin/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /home/jatin/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: fonttools>=4.22.0 in /home/jatin/.local/lib/python3.10/site-packages (from matplotlib) (4.53.1)\n","Requirement already satisfied: cycler>=0.10 in /home/jatin/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /home/jatin/.local/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /home/jatin/.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n","Requirement already satisfied: packaging>=20.0 in /home/jatin/.local/lib/python3.10/site-packages (from matplotlib) (24.1)\n","Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: scikit-learn in /home/jatin/.local/lib/python3.10/site-packages (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jatin/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n","Requirement already satisfied: scipy>=1.6.0 in /home/jatin/.local/lib/python3.10/site-packages (from scikit-learn) (1.14.0)\n","Requirement already satisfied: numpy>=1.19.5 in /home/jatin/.local/lib/python3.10/site-packages (from scikit-learn) (2.0.1)\n","Requirement already satisfied: joblib>=1.2.0 in /home/jatin/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: ipython in /home/jatin/.local/lib/python3.10/site-packages (8.26.0)\n","Requirement already satisfied: traitlets>=5.13.0 in /home/jatin/.local/lib/python3.10/site-packages (from ipython) (5.14.3)\n","Requirement already satisfied: pexpect>4.3 in /usr/lib/python3/dist-packages (from ipython) (4.8.0)\n","Requirement already satisfied: jedi>=0.16 in /home/jatin/.local/lib/python3.10/site-packages (from ipython) (0.19.1)\n","Requirement already satisfied: decorator in /home/jatin/.local/lib/python3.10/site-packages (from ipython) (5.1.1)\n","Requirement already satisfied: pygments>=2.4.0 in /home/jatin/.local/lib/python3.10/site-packages (from ipython) (2.18.0)\n","Requirement already satisfied: matplotlib-inline in /home/jatin/.local/lib/python3.10/site-packages (from ipython) (0.1.7)\n","Requirement already satisfied: typing-extensions>=4.6 in /home/jatin/.local/lib/python3.10/site-packages (from ipython) (4.12.2)\n","Requirement already satisfied: stack-data in /home/jatin/.local/lib/python3.10/site-packages (from ipython) (0.6.3)\n","Requirement already satisfied: exceptiongroup in /home/jatin/.local/lib/python3.10/site-packages (from ipython) (1.2.2)\n","Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/jatin/.local/lib/python3.10/site-packages (from ipython) (3.0.47)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/jatin/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython) (0.8.4)\n","Requirement already satisfied: wcwidth in /home/jatin/.local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n","Requirement already satisfied: asttokens>=2.1.0 in /home/jatin/.local/lib/python3.10/site-packages (from stack-data->ipython) (2.4.1)\n","Requirement already satisfied: pure-eval in /home/jatin/.local/lib/python3.10/site-packages (from stack-data->ipython) (0.2.3)\n","Requirement already satisfied: executing>=1.2.0 in /home/jatin/.local/lib/python3.10/site-packages (from stack-data->ipython) (2.0.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from asttokens>=2.1.0->stack-data->ipython) (1.16.0)\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: pygraphviz in /home/jatin/.local/lib/python3.10/site-packages (1.11)\n"]}],"source":["# Install pandas, numpy, seaborn, and matplotlib\n","! pip install pandas numpy seaborn matplotlib\n","\n","# Install scikit-learn\n","! pip install scikit-learn\n","\n","# Install IPython\n","! pip install ipython\n","\n","# Install pygraphviz\n","! pip install pygraphviz"]},{"cell_type":"markdown","metadata":{},"source":["Please follow the instructions given in the file ````graphviz_installation.txt```` to install graphviz"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-03-11T05:28:50.036182Z","iopub.status.busy":"2023-03-11T05:28:50.034938Z","iopub.status.idle":"2023-03-11T05:28:52.130402Z","shell.execute_reply":"2023-03-11T05:28:52.129014Z","shell.execute_reply.started":"2023-03-11T05:28:50.036127Z"},"trusted":true},"outputs":[],"source":["# import all the necessary libraries here\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score,precision_score,recall_score\n","from graphviz import Digraph\n","from IPython.display import Image, display\n","import pygraphviz as pgv\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>ap_hi</th>\n","      <th>ap_lo</th>\n","      <th>cholesterol</th>\n","      <th>gluc</th>\n","      <th>smoke</th>\n","      <th>alco</th>\n","      <th>active</th>\n","      <th>cardio</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>18393</td>\n","      <td>2</td>\n","      <td>168</td>\n","      <td>62.0</td>\n","      <td>110</td>\n","      <td>80</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20228</td>\n","      <td>1</td>\n","      <td>156</td>\n","      <td>85.0</td>\n","      <td>140</td>\n","      <td>90</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>18857</td>\n","      <td>1</td>\n","      <td>165</td>\n","      <td>64.0</td>\n","      <td>130</td>\n","      <td>70</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17623</td>\n","      <td>2</td>\n","      <td>169</td>\n","      <td>82.0</td>\n","      <td>150</td>\n","      <td>100</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17474</td>\n","      <td>1</td>\n","      <td>156</td>\n","      <td>56.0</td>\n","      <td>100</td>\n","      <td>60</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n","0  18393       2     168    62.0    110     80            1     1      0   \n","1  20228       1     156    85.0    140     90            3     1      0   \n","2  18857       1     165    64.0    130     70            3     1      0   \n","3  17623       2     169    82.0    150    100            1     1      0   \n","4  17474       1     156    56.0    100     60            1     1      0   \n","\n","   alco  active  cardio  \n","0     0       1       0  \n","1     0       1       1  \n","2     0       0       1  \n","3     0       1       1  \n","4     0       0       0  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv('cardio.csv') # Replace with noise/noiseless dataset\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["**Train, Validation, Test split**"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["((6400, 12), (2000, 12), (1600, 12))"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["train_df,test_df = train_test_split(df,test_size=0.2,random_state=1)\n","train_df,val_df = train_test_split(train_df,test_size=0.2,random_state=1)\n","train_df.shape,test_df.shape,val_df.shape"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-03-11T05:29:00.605512Z","iopub.status.busy":"2023-03-11T05:29:00.604855Z","iopub.status.idle":"2023-03-11T05:29:00.619246Z","shell.execute_reply":"2023-03-11T05:29:00.617422Z","shell.execute_reply.started":"2023-03-11T05:29:00.605472Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>gender</th>\n","      <th>height</th>\n","      <th>weight</th>\n","      <th>ap_hi</th>\n","      <th>ap_lo</th>\n","      <th>cholesterol</th>\n","      <th>gluc</th>\n","      <th>smoke</th>\n","      <th>alco</th>\n","      <th>active</th>\n","      <th>cardio</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>18393</td>\n","      <td>2</td>\n","      <td>168</td>\n","      <td>62.0</td>\n","      <td>110</td>\n","      <td>80</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20228</td>\n","      <td>1</td>\n","      <td>156</td>\n","      <td>85.0</td>\n","      <td>140</td>\n","      <td>90</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>18857</td>\n","      <td>1</td>\n","      <td>165</td>\n","      <td>64.0</td>\n","      <td>130</td>\n","      <td>70</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>17623</td>\n","      <td>2</td>\n","      <td>169</td>\n","      <td>82.0</td>\n","      <td>150</td>\n","      <td>100</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>17474</td>\n","      <td>1</td>\n","      <td>156</td>\n","      <td>56.0</td>\n","      <td>100</td>\n","      <td>60</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     age  gender  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n","0  18393       2     168    62.0    110     80            1     1      0   \n","1  20228       1     156    85.0    140     90            3     1      0   \n","2  18857       1     165    64.0    130     70            3     1      0   \n","3  17623       2     169    82.0    150    100            1     1      0   \n","4  17474       1     156    56.0    100     60            1     1      0   \n","\n","   alco  active  cardio  \n","0     0       1       0  \n","1     0       1       1  \n","2     0       0       1  \n","3     0       1       1  \n","4     0       0       0  "]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["### Model Implementation"]},{"cell_type":"markdown","metadata":{},"source":["# How the algorithm works\n","\n","**We'll start with all examples at the root node then:**\n","\n","**We'll calculate information gain for splitting on all possible features and pick the one with the highest value**\n","\n","**Then we'll split the data according to the selected feature**\n","\n","**We'll repeat this  process until stopping criteria is met**\n","\n","## Key Points:\n","\n","### Entropy\n","**Entropy function which is a way to measure impurity**\n","\n","**Entropy is represented by this function**\n","$$H = -\\sum\\limits_{}^{} p_{i}\\text{log}_2 p_{i} \n","$$\n","\n","**Where $(p_1)$ is the fraction of examples that are a certain class**\n","\n","\n","### Information Gain\n","\n","**Information gain is the reduction in entropy when he make a split**\n","\n","**Recall that our goal is to choose the split that gives the highest information gain, information gain equation =**\n","\n","**$$H(p_1^\\text{node})- (w^{\\text{left}}H(p_1^\\text{left}) + w^{\\text{right}}H(p_1^\\text{right}))$$**\n","**where** \n","- $H(p_1^\\text{node})$ is entropy at the node \n","- $H(p_1^\\text{left})$ and $H(p_1^\\text{right})$ are the entropies at the left and the right branches resulting from the split\n","- $w^{\\text{left}}$ and $w^{\\text{right}}$ are the proportion of examples at the left and right branch respectively\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Decision Tree visualization using graphviz"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["graph = pgv.AGraph(strict=True, directed=True)\n","graph2 = pgv.AGraph(strict=True, directed=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Start the Implementation"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["\n","class Node:\n","    def __init__(self, data):\n","        self.data = data\n","        self.left = None\n","        self.right = None\n","\n","class DecisionTree:\n","    def __init__(self):\n","        self.root = Node(None)\n","        self.height = -1\n","\n","    def find_splits(self, data, column_index):\n","        \"\"\"\n","        Identifies potential split points for a given feature.\n","        \n","        Parameters:\n","            data (numpy.ndarray): The dataset used for finding splits.\n","            column_index (int): The index of the column for which to find potential splits.\n","        \n","        Returns:\n","            potential_splits (list): A list of potential split points for the specified feature.\n","        \"\"\"\n","\n","        x = data[:,:-1]\n","        potential_splits = []\n","        \n","        ### START CODE HERE ###\n","        unique_values = np.unique(x)\n","        for i in range(len(unique_values) - 1):\n","            midpoint = (unique_values[i] + unique_values[i + 1]) / 2\n","            potential_splits.append(midpoint)\n","\n","        ### END CODE HERE ###\n","\n","        return potential_splits\n","\n","\n","    def calculate_entropy(self, data):\n","        \"\"\"\n","        Calculates the entropy of the dataset.\n","        \n","        Parameters:\n","            data (numpy.ndarray): The dataset for which to calculate entropy.\n","        \n","        Returns:\n","            entropy (float): The entropy value of the dataset.\n","        \"\"\"\n","        y = data[:, -1]\n","        _, counts = np.unique(y, return_counts=True)\n","        probabilities = counts / counts.sum()\n","        ### START CODE HERE ###\n","        entropy = None # Replace with actual code\n","        entropy = -np.sum(probabilities * np.log2(probabilities))\n","        ### END CODE HERE ###\n","        return entropy\n","\n","    def calculate_information_gain(self, data, column_no, value):\n","        \"\"\"\n","        Calculates the information gain resulting from splitting the data on a specific feature at a specific value.\n","        \n","        Parameters:\n","            data (numpy.ndarray): The dataset to split.\n","            column_no (int): The index of the feature used to split the data.\n","            value (float): The value at which to split the feature.\n","        \n","        Returns:\n","            information_gain (float): The information gain from the split.\n","        \"\"\"\n","        parent_entropy = self.calculate_entropy(data)\n","        left_data = data[data[:, column_no] <= value]\n","        right_data = data[data[:, column_no] > value]\n","\n","        n = len(left_data) + len(right_data)\n","        p_left_data = len(left_data) / n\n","        p_right_data = len(right_data) / n\n","\n","        ### START CODE HERE ###\n","\n","        child_entropy = None  # Replace with actual code\n","        child_entropy = (p_left_data * self.calculate_entropy(left_data) + p_right_data * self.calculate_entropy(right_data))\n","        \n","        information_gain = None  # Replace with actual code\n","        information_gain = parent_entropy - child_entropy\n","        ### END CODE HERE ###\n","\n","        return information_gain\n","\n","    def majority(self, data):\n","        \"\"\"\n","        Determines the majority class label in the dataset.\n","        \n","        Parameters:\n","            data (pandas.DataFrame): The dataset to classify.\n","        \n","        Returns:\n","            majority_class (int): The label of the majority class.\n","        \"\"\"\n","        ### START CODE HERE ###\n","        cls, count = np.unique(data[\"cardio\"], return_counts=True) #Replace target with actual target attribute\n","        \n","        ### END CODE HERE ###\n","        return cls[np.argmax(count)]\n","\n","    def classify(self, data, edge):\n","        \"\"\"\n","        Classifies a dataset as a leaf node.\n","        \n","        Parameters:\n","            data (pandas.DataFrame): The dataset to classify.\n","            edge (int): The index of the parent edge.\n","        \n","        Returns:\n","            leaf (Node): A leaf node with classification information.\n","        \"\"\"\n","        classification = self.majority(data)\n","        entropy = self.calculate_entropy(data.values)\n","        d = {\"ID\": \"Leaf\", \"Classification\": classification, \"Parent_Edge\": edge, \"Entropy\": entropy, \"Samples\": data.shape[0]}\n","        leaf = Node(d)\n","        return leaf\n","    \n","    def build_tree(self, data, max_depth, attributes, edge, height):\n","        \"\"\"\n","        A recursive utility function for building the decision tree.\n","        \n","        Parameters:\n","            data (pandas.DataFrame): The dataset to build the tree from.\n","            max_depth (int): The maximum allowed depth of the tree.\n","            attributes (list): The list of attributes used in the dataset.\n","            edge (int): The index of the parent edge.\n","            height (int): The current height of the tree.\n","        \n","        Returns:\n","            node (Node): The root node of the subtree created.\n","        \"\"\"\n","        ### START CODE HERE ### \n","\n","        # CODE HERE ... Implement the logic to extract the leaf node\n","        if height == max_depth or len(np.unique(data.iloc[:, -1])) == 1 or len(attributes) == 0:\n","            print(\"Reached Leaf\")\n","            return self.classify(data, edge)   \n","            #return leaf\n","            \n","        ### END CODE HERE ###\n","        \n","        \n","        best = {\"ID\": \"\", \"best_attribute\": \"\", \"best_gain\": -1, \"best_split\": -1, \"best_entropy\": -1}\n","\n","\n","        ### START CODE HERE ###\n","\n","        # CODE TO EXTRACT THE BEST ATTRIBUTE AND STORE IT IN best VARIABLE\n","        for attribute in attributes:\n","            column_no = data.columns.get_loc(attribute)\n","            potential_splits = self.find_splits(data.values, column_no)\n","            for split in potential_splits:\n","                gain = self.calculate_information_gain(data.values, column_no, split)\n","                if gain > best[\"best_gain\"]:\n","                    best[\"ID\"] = attribute\n","                    best[\"best_attribute\"] = attribute\n","                    best[\"best_gain\"] = gain\n","                    best[\"best_split\"] = split\n","                    best[\"best_entropy\"] = self.calculate_entropy(data.values)\n","        ### END HERE ###\n","\n","        _, sample = np.unique(data[\"cardio\"], return_counts=True)\n","        d = {\"ID\": best[\"best_attribute\"], \"Entropy\": best[\"best_entropy\"], \n","             \"Samples\": data.shape[0], \"Parent_Edge\": edge, \n","             \"Best_Split\": best[\"best_split\"], \"Values\": sample}\n","        node = Node(d)\n","        node.left = self.build_tree(data[data[best[\"best_attribute\"]] <= best[\"best_split\"]], max_depth, attributes, 2 * edge + 1, height + 1)\n","        node.right = self.build_tree(data[data[best[\"best_attribute\"]] > best[\"best_split\"]], max_depth, attributes, 2 * edge + 2, height + 1)\n","\n","        root = f'{d[\"ID\"]} <= {d[\"Best_Split\"]}\\nEntropy = {d[\"Entropy\"]}\\nSamples = {d[\"Samples\"]}\\nValues = {sample}'\n","        \n","        graph.add_node(str(edge), label=root)\n","        graph.add_edge(str(edge), str(2 * edge + 1))\n","        graph.add_edge(str(edge), str(2 * edge + 2))\n","        return node\n","\n","    def fit(self, data, max_depth=100):\n","        \"\"\"\n","        Fits the decision tree model to the provided dataset.\n","        \n","        Parameters:\n","            data (pandas.DataFrame): The dataset to fit the tree to.\n","            max_depth (int): The maximum allowed depth of the tree.\n","        \n","        Returns:\n","            None\n","        \"\"\"\n","        print(\"Building Tree\")\n","        attributes = data.columns.tolist()[:-1]\n","        self.attributes = np.array(attributes)\n","        self.root = self.build_tree(data, max_depth, attributes, 0, 0)\n","\n","        \n","    def plt(self, graph, node, vert):\n","        \"\"\"\n","        Plots the decision tree using a graph representation.\n","        \n","        Parameters:\n","            graph (Graph): The graph object to use for plotting.\n","            node (Node): The current node in the tree.\n","            vert (int): The current vertex in the graph.\n","        \n","        Returns:\n","            root (str): The label of the current node.\n","        \"\"\"\n","        d = node.data\n","        if \"Classification\" in node.data.keys():\n","            root = f'{d[\"ID\"]}\\nEntropy = {d[\"Entropy\"]}\\nSamples = {d[\"Samples\"]}\\nClass = {d[\"Classification\"]}'\n","            graph.add_node(str(vert), label=root)\n","            return root\n","        \n","        root = f'{d[\"ID\"]} <= {d[\"Best_Split\"]}\\nEntropy = {d[\"Entropy\"]}\\nSamples = {d[\"Samples\"]}\\nValues = {d[\"Values\"]}'\n","        graph.add_node(str(vert), label=root)\n","        root1 = self.plt(graph, node.left, 2 * vert + 1)\n","        graph.add_node(str(2 * vert + 1), label=root1)\n","        root2 = self.plt(graph, node.right, 2 * vert + 2)\n","        graph.add_node(str(2 * vert + 2), label=root2)\n","        \n","        graph.add_edge(str(vert), str(2 * vert + 1))\n","        graph.add_edge(str(vert), str(2 * vert + 2))\n","        return root\n","\n","    def prune_util(self, node):\n","        \"\"\"\n","        Utility function for pruning a node in the decision tree.\n","        \n","        Parameters:\n","            node (Node): The node to prune.\n","        \n","        Returns:\n","            d (dict): The data for the pruned leaf node.\n","        \"\"\"\n","        d = {\"ID\": \"Leaf\", \"Classification\": 0, \"Parent_Edge\": node.data[\"Parent_Edge\"], \n","             \"Entropy\": 0, \"Samples\": node.data[\"Samples\"]}\n","        if node.data[\"Values\"][0] > node.data[\"Values\"][1]:\n","            d[\"Classification\"] = 0\n","        else:\n","            d[\"Classification\"] = 1\n","        return d\n","        \n","    def prune(self, val_df, node):\n","        \"\"\"\n","        Prunes the decision tree to avoid overfitting.\n","        \n","        Parameters:\n","            val_df (pandas.DataFrame): The validation dataset used to evaluate pruning.\n","            node (Node): The current node to consider pruning.\n","        \n","        Returns:\n","            None\n","        \"\"\"\n","        if \"Classification\" in node.data.keys():\n","            return\n","        \n","        ### START CODE HERE ###\n","\n","        # RECURSIVE CALLS TO IT'S CHILDREN\n","        if node.left is not None:\n","            self.prune(val_df, node.left)\n","        if node.right is not None:\n","            self.prune(val_df, node.right)\n","        ### END CODE HERE ###\n","        \n","        curr_val = accuracy_score(val_df.values[:, -1], self.predict(val_df.values[:, :-1]))\n","\n","        tmp1 = node.left\n","        tmp2 = node.right\n","        tmp = node.data\n","        node.left = None\n","        node.right = None\n","\n","        node.data = self.prune_util(node)\n","\n","        new_val = accuracy_score(val_df.values[:, -1], self.predict(val_df.values[:, :-1]))\n","\n","        # Decide whether to keep the pruning or revert to the original node\n","\n","        ### START CODE HERE ###\n","        if new_val >= curr_val:\n","            pass\n","        # Implement the decision logic for keeping or reverting pruning\n","        else:\n","            node.left = tmp1\n","            node.right = tmp2\n","            node.data = tmp\n","        ### END CODE HERE ###\n","\n","    def predict_one(self, data):\n","        \"\"\"\n","        Predicts the class label for a single data point.\n","        \n","        Parameters:\n","            data (numpy.ndarray): The data point to classify.\n","        \n","        Returns:\n","            classification (int): The predicted class label.\n","        \"\"\"\n","        node = self.root\n","        while \"Classification\" not in node.data.keys():\n","            d = node.data\n","            if data[np.argwhere(self.attributes == d[\"ID\"]).squeeze()] <= d[\"Best_Split\"]:\n","                node = node.left\n","            else:\n","                node = node.right\n","        return node.data[\"Classification\"]\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predicts the class labels for a dataset.\n","        \n","        Parameters:\n","            X (numpy.ndarray): The dataset to classify.\n","        \n","        Returns:\n","            Y_pred (numpy.ndarray): The predicted class labels.\n","        \"\"\"\n","        ### START CODE HERE ###\n","        predictions = []\n","        for data_point in X:\n","            prediction = self.predict_one(data_point)\n","            predictions.append(prediction)\n","        return np.array(predictions)\n","        ### END CODE HERE ###"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["<__main__.DecisionTree object at 0x7b1b15cba4a0>\n","Building Tree\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m DecisionTree()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[28], line 194\u001b[0m, in \u001b[0;36mDecisionTree.fit\u001b[0;34m(self, data, max_depth)\u001b[0m\n\u001b[1;32m    192\u001b[0m attributes \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(attributes)\n\u001b[0;32m--> 194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[28], line 156\u001b[0m, in \u001b[0;36mDecisionTree.build_tree\u001b[0;34m(self, data, max_depth, attributes, edge, height)\u001b[0m\n\u001b[1;32m    154\u001b[0m potential_splits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfind_splits(data\u001b[38;5;241m.\u001b[39mvalues, column_no)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m potential_splits:\n\u001b[0;32m--> 156\u001b[0m     gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_information_gain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_no\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gain \u001b[38;5;241m>\u001b[39m best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_gain\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    158\u001b[0m         best[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m attribute\n","Cell \u001b[0;32mIn[28], line 80\u001b[0m, in \u001b[0;36mDecisionTree.calculate_information_gain\u001b[0;34m(self, data, column_no, value)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m### START CODE HERE ###\u001b[39;00m\n\u001b[1;32m     79\u001b[0m child_entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Replace with actual code\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m child_entropy \u001b[38;5;241m=\u001b[39m (p_left_data \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_entropy(left_data) \u001b[38;5;241m+\u001b[39m p_right_data \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright_data\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     82\u001b[0m information_gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Replace with actual code\u001b[39;00m\n\u001b[1;32m     83\u001b[0m information_gain \u001b[38;5;241m=\u001b[39m parent_entropy \u001b[38;5;241m-\u001b[39m child_entropy\n","Cell \u001b[0;32mIn[28], line 50\u001b[0m, in \u001b[0;36mDecisionTree.calculate_entropy\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     48\u001b[0m y \u001b[38;5;241m=\u001b[39m data[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     49\u001b[0m _, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 50\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m counts \u001b[38;5;241m/\u001b[39m \u001b[43mcounts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m### START CODE HERE ###\u001b[39;00m\n\u001b[1;32m     52\u001b[0m entropy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# Replace with actual code\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/_core/_methods.py:52\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model = DecisionTree()\n","print(model)\n","model.fit(train_df)"]},{"cell_type":"markdown","metadata":{},"source":["### Plotting the Decision Tree before Pruning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["graph.draw(\"Decision_Tree_Before_Pruning.png\", prog=\"dot\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["display(Image(filename='Decision_Tree_Before_Pruning.png'))"]},{"cell_type":"markdown","metadata":{},"source":["### Testing before Pruning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_test = test_df.values[:,:-1]\n","Y_test = test_df.values[:,-1]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Y_pred = model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Accuracy,Macro Precision, Macro Recall Before Pruning\")\n","accuracy_score(Y_test,Y_pred),precision_score(Y_test,Y_pred,average='macro'),recall_score(Y_test,Y_pred,average = 'macro')"]},{"cell_type":"markdown","metadata":{},"source":["### Reduced Error Pruning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.prune(val_df,model.root)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_ =model.plt(graph2,model.root,0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["graph2.draw(\"Decision_Tree_After_Pruning.png\", prog=\"dot\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["display(Image(filename='Decision_Tree_After_Pruning.png'))"]},{"cell_type":"markdown","metadata":{},"source":["### Testing after Pruning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Y_pred = model.predict(X_test)\n","print(\"Accuracy,Macro Precision, Macro Recall After Pruning\")\n","accuracy_score(Y_test,Y_pred),precision_score(Y_test,Y_pred,average='macro'),recall_score(Y_test,Y_pred,average = 'macro')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1829286,"sourceId":2984728,"sourceType":"datasetVersion"}],"dockerImageVersionId":30260,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
