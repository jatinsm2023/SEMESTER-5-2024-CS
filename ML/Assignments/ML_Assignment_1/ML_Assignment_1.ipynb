{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emjAQ_34SozP",
        "tags": []
      },
      "source": [
        "# Programming Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKkKCHwsSozQ"
      },
      "source": [
        "In this programming assignment, you will implement a linear regression model and a logistic regression model.\n",
        "\n",
        "In Part 1, you have to implement a linear regression model to predict the price of a house based on various input features.\n",
        "\n",
        "In Part 2, you have to implement a logistic regression model to predict the species of a grain using various morphological features.\n",
        "\n",
        "The assignment zip file (ML_Assignment_1.zip) contains 4 datasets which will be used in this assignment.\n",
        "\n",
        "You have to write your code in this jupyter notebook and submit the solved jupyter notebook with the file name \\<Roll_No\\>_A1.ipynb for evaluation. You have to enter your code only in those cells which are marked as ```## CODE REQUIRED ##```, and you have to write your code only between ```### START CODE HERE ###``` and ```### END CODE HERE ###``` comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHPf8pHpSozR"
      },
      "source": [
        "## Part 1: Linear Regression\n",
        "\n",
        "### Problem Statement  \n",
        "A real estate company is building a machine learning model to determine the price of a house. The model will take various information regarding a house as input features and predict the price per unit area. They decided to use the linear regression as the machine learning model. Your task is to help the company to build the model.\n",
        "Given various features of a house, you will create a linear regression model to predict the price of the house.\n",
        "\n",
        "### Data Description\n",
        "\n",
        "**For Even Roll Number Students:**\n",
        "\n",
        "Dataset Filename: Taiwan_House.csv\n",
        "\n",
        "Attributes Information:\n",
        "1. Transaction date\n",
        "2. House age\n",
        "3. Distance to the nearest MRT station\n",
        "4. Number of convenience stores\n",
        "5. Latitude\n",
        "6. Longitude\n",
        "\n",
        "Target variable: house price of unit area\n",
        "\n",
        "**For Odd Roll Number Students:**\n",
        "\n",
        "Dataset Filename: Boston_House.csv\n",
        "\n",
        "Attributes Information:\n",
        "1. CRIM: per capita crime rate by town\n",
        "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "3. INDUS: proportion of non-­retail business acres per town\n",
        "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "5. NOX: nitric oxides concentration (parts per 10 million)\n",
        "6. RM: average number of rooms per dwelling\n",
        "7. AGE: proportion of owner­occupied units built prior to 1940\n",
        "8. DIS: weighted distances to five Boston employment centres\n",
        "9. RAD: index of accessibility to radial highways\n",
        "10. TAX: full-­value property-­tax rate per $10,000\n",
        "\n",
        "11. PTRATIO: pupil-­teacher ratio by town\n",
        "12. B: 1000(Bk ­- 0.63)^2 where Bk is the proportion of blacks by town\n",
        "13. LSTAT: % lower status of the population\n",
        "\n",
        "Target Variable: MEDV: Median value of owner-­occupied homes in $1000's\n",
        "\n",
        "\n",
        "These are the following steps or functions that you have to complete to create and train the linear regression model:\n",
        "1. Reading the data\n",
        "2. Computing the loss function\n",
        "3. Computing the gradient of the loss\n",
        "4. Training the model using Batch Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rJDNL8y8SozR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ExYIXiSozS"
      },
      "source": [
        "### 1.1. Reading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgnxuEDkSozS"
      },
      "source": [
        "In the following function ```load_data```, you have to read the data from the file and store the data into a pandas dataframe. Then you have to create two numpy arrays $X$ and $y$ from the dataframe:\n",
        "\n",
        "+ $X$: Input data of the shape (number of samples, number of input features)\n",
        "+ $y$: Target variable of the shape (number of samples,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "DEal5SFmSozS",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X:  (414, 7) Shape of y:  (414,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    This function loads the data into a pandas dataframe and coverts it into X and y numpy arrays\n",
        "\n",
        "    Args:\n",
        "        filepath: File path as a string\n",
        "    Returns:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "        y: Target variable of the shape (# of sample,)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    df = pd.read_csv(filepath)\n",
        "    attributes = ['transaction date', 'house age', 'distance to the nearest MRT station', 'number of convenience stores', 'latitude', 'longitude', 'house price of unit area']\n",
        "    X = df[attributes].to_numpy()\n",
        "    y = df['house price of unit area'].to_numpy()\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X,y\n",
        "\n",
        "filepath = None\n",
        "### START CODE HERE ###\n",
        "## set the file path\n",
        "filepath = 'Taiwan_House.csv'\n",
        "### END CODE HERE ###\n",
        "X, y = load_data(filepath)\n",
        "\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se3Qrch5PoIO"
      },
      "source": [
        "We will not use all the features from ```X```.\n",
        "\n",
        "**For Even Roll Number Students:**\n",
        "Set the last two digits of your roll number as the random seed and pick a number ``r`` between 3 and 6 (both inclusive) randomly. Use the first ```r``` features of the numpy array ```X```.\n",
        "\n",
        "**For Odd Roll Number Students:**\n",
        "Set the last two digits of your roll number as the random seed and pick a number ``r`` between 9 and 13 (both inclusive) randomly. Use the first ```r``` features of the numpy array ```X```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "mnRd0p8LtRJ5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X:  (414, 3) Shape of y:  (414,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def random_feature_selection(X):\n",
        "    \"\"\"\n",
        "    For Even Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r\n",
        "    between 3 and 6 randomly. Use the first ```r``` features of the numpy array X.\n",
        "    For Odd Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r\n",
        "    between 9 and 13 randomly. Use the first ```r``` features of the numpy array X.\n",
        "    Args:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "    Returns:\n",
        "        X_new: New input data of the shape (# of samples, r) containg only the first r features from X\n",
        "    \"\"\"\n",
        "\n",
        "    X_new = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    random.seed(32)\n",
        "    r = random.randint(3,6)\n",
        "\n",
        "    X_new = X[:, :r]\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X_new\n",
        "\n",
        "X = random_feature_selection(X)\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAc8UixtqLS"
      },
      "source": [
        "We need to pre-process the data. We are using min-max scaler to scale the input data ($X$).\n",
        "\n",
        "After that, we split the data (```X``` and ```y```) into a training dataset (```X_train``` and ```y_train```) and test dataset (```X_test``` and ```y_test```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "LLmD1I3-SozT",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train:  (310, 3) Shape of y_train:  (310,)\n",
            "Shape of X_test:  (104, 3) Shape of y_test:  (104,)\n"
          ]
        }
      ],
      "source": [
        "## Data scaling and train-test split\n",
        "\n",
        "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_index = int(X.shape[0] * (1 - test_size))\n",
        "\n",
        "    train_indices = indices[:split_index]\n",
        "    test_indices = indices[split_index:]\n",
        "\n",
        "    X_train = X[train_indices]\n",
        "    X_test = X[test_indices]\n",
        "    y_train = y[train_indices]\n",
        "    y_test = y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def min_max_scaler(X, feature_range=(0, 1)):\n",
        "    X_min = np.min(X, axis=0)\n",
        "    X_max = np.max(X, axis=0)\n",
        "\n",
        "    X_scaled = (X-X_min)/(X_max-X_min)\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "X = min_max_scaler(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n",
        "print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrFEUV2-SozU"
      },
      "source": [
        "### 1.2. Computing the Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1BC0TPCSozU"
      },
      "source": [
        "In linear regression, the model parameters are:\n",
        "\n",
        "+ $w$: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "\n",
        "+ $b$: Bias parameter (scalar) of the linear regression model\n",
        "\n",
        "Both $w$ and $b$ are numpy arrays.\n",
        "\n",
        "Given the model parameters $w$ and $b$, the prediction for an input sample $X^i$ is:\n",
        "$$h_{w,b}(X^i) = w \\cdot X^i + b$$\n",
        "where $X^i$ is the $i^{th}$ training sample with shape (number of features,1)\n",
        "\n",
        "For linear regression, you have to implement and compute Mean Squarred Error loss fucntion:\n",
        "$$ L_{w,b}(X) = \\sum_{i=1}^{m}(y^i - h_{w,b}(X^i))^2 $$\n",
        "where $y^i$ is the true target value for the $i^{th}$ sample and $h_{w,b}(X^i)$ is the predicted value for the $i^{th}$ sample using the parameters $w$ and $b$.\n",
        "\n",
        "$w$ is the list of parameters excluding the bias and $b$ is the bias term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "t6GqZMlYSozV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def loss_function(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b: Bias parameter (scalar) of the linear regression model\n",
        "\n",
        "    Returns\n",
        "        loss: The loss function value of using w and b as the parameters to fit the data points in X and y\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = X.shape[0]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    loss = 0\n",
        "    for i in range(m):\n",
        "        loss = loss+((np.dot(w,X[i])+b)-y[i])**2\n",
        "    loss = loss/(2*m)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0h7jhxGSozV"
      },
      "source": [
        "### 1.3. Comptuing the Gradient of the Loss\n",
        "\n",
        "In this following function ```compute_gradient```, you have to compute the gradients $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ of the loss $L$ w.r.t. $w$ and $b$. More specifically, you have to iterate over every training example and compute the gradients of the loss for that training example. Finally, aggregate the gradient values for all the training examples and take the average. The gradients can be computed as:\n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (h_{w,b}(X^i)-y^i)X^i$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{w,b}(X^i)-y^i)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "do-IiCTZSozV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient values\n",
        "    Args:\n",
        "       X: Input data of the shape (# of training samples, # of input features)\n",
        "       y: Target variable of the shape (# of training sample,)\n",
        "       w: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "       b: Bias parameter of the linear regression model of the shape (1,1) or a scaler\n",
        "    Returns:\n",
        "       dL_dw : The gradient of the cost w.r.t. the parameters w with shape same as w\n",
        "       dL_db : The gradient of the cost w.r.t. the parameter b with shape same as b\n",
        "    \"\"\"\n",
        "\n",
        "    # Number of training examples\n",
        "    m = X.shape[0]\n",
        "    k = w.shape[0]\n",
        "    dL_dw = None\n",
        "    dL_db = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    dL_dw = np.zeros(k)  \n",
        "    dL_db = 0\n",
        "    for i in range(m):\n",
        "      delta = (np.dot(w,X[i])+b)-y[i]\n",
        "      for j in range(k):\n",
        "        dL_dw[j] += delta*X[i][j]\n",
        "      dL_db += delta\n",
        "   \n",
        "    dL_dw = dL_dw/m\n",
        "    dL_db = dL_db/m\n",
        "    ### END CODE HERE ###\n",
        "    return dL_dw, dL_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYL1jWaZSozV"
      },
      "source": [
        "### 1.4. Training the Model using Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vwyRzWqSozV"
      },
      "source": [
        "Finally, you have to implement the batch gradient descent algorithm to train and learn the parameters of the linear regression model. You have to use ```loss_function``` and ```compute_gradient``` functions that you have implemented earlier in this assignment.\n",
        "\n",
        "In this ```batch_gradient_descent``` function, you have to compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n",
        "\n",
        "+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
        "\n",
        "+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "Additionally, you have compute the loss function values in every iteration and store it in the list variable ```loss_hist``` and print the loss value after every 100 iterations during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "UHKldb3hSozV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def batch_gradient_descent(X, y, w_initial, b_initial, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Batch gradient descent to learn the parameters (w and b) of the linear regression model and to print loss values\n",
        "    every 100 iterations\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w_initial: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b_initial: Initial bias parameter (scalar) of the linear regression model\n",
        "        alpha: Learning rate\n",
        "        num_iters: number of iterations\n",
        "    Returns\n",
        "        w: Updated values of parameters of the model after training\n",
        "        b: Updated bias of the model after training\n",
        "        loss_hist: List of loss values for every iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # number of training examples\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # to store loss values for every iteation as a list and print loss value after every 100 iterations\n",
        "    loss_hist = []\n",
        "\n",
        "    # Initialize parameters\n",
        "    w = copy.deepcopy(w_initial) ## deepcopy is used so that the updates do not change the initial variable values\n",
        "    b = b_initial\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    for i in range(num_iters):\n",
        "        dL_dw, dL_db = compute_gradient(X, y, w, b)\n",
        "        w = w - alpha*dL_dw\n",
        "        b = b - alpha*dL_db\n",
        "        loss = loss_function(X, y, w, b)\n",
        "        loss_hist.append(loss)\n",
        "        if i%100 == 0:\n",
        "            print('Loss at iteration', i, 'is', loss)\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return w, b, loss_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0h3DUMwNocp"
      },
      "source": [
        "Now you have to intialize the model parameters ($w$ and $b$) and learning rate (```alpha```). The learning rate ```alpha``` has to be randomly initialized between 0.0001 and 0.001. For the learning rate, you have to first set the last two digits of your roll number as the random seed using ```random.seed()``` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Xd8QFReNDyGs"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "## set the last two digits of your roll number as the random seed\n",
        "random_seed = None\n",
        "### START CODE HERE ###\n",
        "random_seed = 32\n",
        "### END CODE HERE ###\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    This function randomly initializes the model parameters (w and b) and the hyperparameter alpha\n",
        "    Initial w and b should be randomly sampled from a normal distribution with mean 0\n",
        "    alpha should be randomly initialized between 0.0001 and 0.001 by using last two digits of your roll number as the random seed\n",
        "    Args:\n",
        "        None\n",
        "    Returns:\n",
        "        initial_w: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        initial_b: Initial bias parameter (scalar) of the linear regression model\n",
        "        alpha: Learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    initial_w = None\n",
        "    initial_b = None\n",
        "    alpha = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    initial_w = np.random.normal(size=(X_train.shape[1]))\n",
        "    initial_b = np.random.normal()\n",
        "    alpha = np.random.uniform(0.0001,0.001)\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return initial_w,initial_b,alpha\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7nt5VdORUEu"
      },
      "source": [
        "In the next cell, the model is trained using batch gradient descent algorithm for ```num_iters=10000``` iterations. You can change the number of iterations to check any improvements in the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "9rAYubtASozV",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss at iteration 0 is 781.0394233491434\n",
            "Loss at iteration 100 is 644.4113188429702\n",
            "Loss at iteration 200 is 535.3154733113483\n",
            "Loss at iteration 300 is 448.1873437029223\n",
            "Loss at iteration 400 is 378.58714177706185\n",
            "Loss at iteration 500 is 322.9724879860524\n",
            "Loss at iteration 600 is 278.517018717242\n",
            "Loss at iteration 700 is 242.965658364859\n",
            "Loss at iteration 800 is 214.5191451873272\n",
            "Loss at iteration 900 is 191.74189789666187\n",
            "Loss at iteration 1000 is 173.4885051291298\n",
            "Loss at iteration 1100 is 158.8450735632538\n",
            "Loss at iteration 1200 is 147.0824313140018\n",
            "Loss at iteration 1300 is 137.61879030196417\n",
            "Loss at iteration 1400 is 129.98995565950597\n",
            "Loss at iteration 1500 is 123.82555669498826\n",
            "Loss at iteration 1600 is 118.83008228045492\n",
            "Loss at iteration 1700 is 114.76774954701325\n",
            "Loss at iteration 1800 is 111.45043106328966\n",
            "Loss at iteration 1900 is 108.72802228726609\n",
            "Loss at iteration 2000 is 106.48075604022897\n",
            "Loss at iteration 2100 is 104.61307045219905\n",
            "Loss at iteration 2200 is 103.04871637641025\n",
            "Loss at iteration 2300 is 101.72685373956085\n",
            "Loss at iteration 2400 is 100.59893693473553\n",
            "Loss at iteration 2500 is 99.62622976818494\n",
            "Loss at iteration 2600 is 98.77782270855012\n",
            "Loss at iteration 2700 is 98.02905090838696\n",
            "Loss at iteration 2800 is 97.36023199007707\n",
            "Loss at iteration 2900 is 96.75565896231143\n",
            "Loss at iteration 3000 is 96.20279669772468\n",
            "Loss at iteration 3100 is 95.69164082596959\n",
            "Loss at iteration 3200 is 95.21420621327451\n",
            "Loss at iteration 3300 is 94.76411883523606\n",
            "Loss at iteration 3400 is 94.33629014402044\n",
            "Loss at iteration 3500 is 93.92665725542288\n",
            "Loss at iteration 3600 is 93.53197565164774\n",
            "Loss at iteration 3700 is 93.14965378483441\n",
            "Loss at iteration 3800 is 92.77762111195416\n",
            "Loss at iteration 3900 is 92.41422280360895\n",
            "Loss at iteration 4000 is 92.05813573515334\n",
            "Loss at iteration 4100 is 91.70830145835413\n",
            "Loss at iteration 4200 is 91.36387272131923\n",
            "Loss at iteration 4300 is 91.02417079819487\n",
            "Loss at iteration 4400 is 90.68865144365597\n",
            "Loss at iteration 4500 is 90.35687772886702\n",
            "Loss at iteration 4600 is 90.02849836796604\n",
            "Loss at iteration 4700 is 89.70323042527488\n",
            "Loss at iteration 4800 is 89.38084551776437\n",
            "Loss at iteration 4900 is 89.06115880627999\n",
            "Loss at iteration 5000 is 88.74402021183708\n",
            "Loss at iteration 5100 is 88.42930740723831\n",
            "Loss at iteration 5200 is 88.11692022516388\n",
            "Loss at iteration 5300 is 87.80677619642938\n",
            "Loss at iteration 5400 is 87.49880698996851\n",
            "Loss at iteration 5500 is 87.19295557227845\n",
            "Loss at iteration 5600 is 86.88917394090312\n",
            "Loss at iteration 5700 is 86.58742131592564\n",
            "Loss at iteration 5800 is 86.28766269689363\n",
            "Loss at iteration 5900 is 85.98986771131331\n",
            "Loss at iteration 6000 is 85.69400969577883\n",
            "Loss at iteration 6100 is 85.40006496271565\n",
            "Loss at iteration 6200 is 85.10801221521922\n",
            "Loss at iteration 6300 is 84.81783208005818\n",
            "Loss at iteration 6400 is 84.52950673495477\n",
            "Loss at iteration 6500 is 84.24301961109033\n",
            "Loss at iteration 6600 is 83.95835515563024\n",
            "Loss at iteration 6700 is 83.67549864213662\n",
            "Loss at iteration 6800 is 83.3944360191913\n",
            "Loss at iteration 6900 is 83.1151537895065\n",
            "Loss at iteration 7000 is 82.83763891336044\n",
            "Loss at iteration 7100 is 82.56187873144279\n",
            "Loss at iteration 7200 is 82.28786090318745\n",
            "Loss at iteration 7300 is 82.01557335746263\n",
            "Loss at iteration 7400 is 81.74500425312141\n",
            "Loss at iteration 7500 is 81.47614194742093\n",
            "Loss at iteration 7600 is 81.2089749707196\n",
            "Loss at iteration 7700 is 80.94349200618417\n",
            "Loss at iteration 7800 is 80.6796818734967\n",
            "Loss at iteration 7900 is 80.41753351575086\n",
            "Loss at iteration 8000 is 80.1570359888954\n",
            "Loss at iteration 8100 is 79.89817845321058\n",
            "Loss at iteration 8200 is 79.64095016640684\n",
            "Loss at iteration 8300 is 79.38534047801862\n",
            "Loss at iteration 8400 is 79.13133882483348\n",
            "Loss at iteration 8500 is 78.87893472714491\n",
            "Loss at iteration 8600 is 78.62811778566773\n",
            "Loss at iteration 8700 is 78.37887767897799\n",
            "Loss at iteration 8800 is 78.13120416137585\n",
            "Loss at iteration 8900 is 77.88508706108391\n",
            "Loss at iteration 9000 is 77.64051627871628\n",
            "Loss at iteration 9100 is 77.39748178596236\n",
            "Loss at iteration 9200 is 77.15597362444376\n",
            "Loss at iteration 9300 is 76.91598190470945\n",
            "Loss at iteration 9400 is 76.67749680534268\n",
            "Loss at iteration 9500 is 76.4405085721566\n",
            "Loss at iteration 9600 is 76.20500751746255\n",
            "Loss at iteration 9700 is 75.9709840193956\n",
            "Loss at iteration 9800 is 75.73842852128823\n",
            "Loss at iteration 9900 is 75.5073315310811\n",
            "Updated w:  [10.34628493  4.9813521  -7.53382499]\n",
            "Updated b:  31.033947295965024\n"
          ]
        }
      ],
      "source": [
        "# initialize the parameters and hyperparameter\n",
        "initial_w, initial_b, alpha = initialize_parameters()\n",
        "\n",
        "# number of iterations\n",
        "num_iters = 10000\n",
        "\n",
        "w,b,loss_hist = batch_gradient_descent(X_train ,y_train, initial_w, initial_b, alpha, num_iters)\n",
        "print(\"Updated w: \",w)\n",
        "print(\"Updated b: \",b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sluek2LkSozW"
      },
      "source": [
        "### 1.5. Final Train Error and Test Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH9oHC10fAlV"
      },
      "source": [
        "After the linear regression model is trained, we will compute the final train error and test error for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "UIXb7sSvSozW",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Error:  75.27997295772428 , Test Error:  117.13349992709517\n"
          ]
        }
      ],
      "source": [
        "## Train and Test error computation\n",
        "\n",
        "train_error = loss_function(X_train,y_train,w,b)\n",
        "test_error = loss_function(X_test,y_test,w,b)\n",
        "print(\"Train Error: \",train_error, \", Test Error: \",test_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eximd12PbAgC"
      },
      "source": [
        "### 1.6. Plotting the loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meDwLCCIfOBo"
      },
      "source": [
        "We will plot the loss function values for every training iteration. If the model is trained properly, you will see that the loss function reduces as the training progesses and it converges at some point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "DW4bue0oSozW"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# PLotting the loss values for every training iterations\n",
        "\n",
        "loss_plot = [loss_hist[i] for i in range(len(loss_hist))]\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss function\")\n",
        "plt.plot(loss_plot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAVgq4HWCmDX"
      },
      "source": [
        "### 1.7 Experimenting with different values of the Hyperparemeters\n",
        "\n",
        "Previously, we randomly sampled the learning rate and the number of features to train the model. Now, you have to manually choose the number of features and the learning rate. Then, you have to train the model again on the manually choosen hyperparameters (number of features and learning rate). In the next cell, you have to manually choose the hyperparameters and write the code to train the model.\n",
        "\n",
        "After the model is trained, you have to compare the performance of the model with random chosen hyperparameters and manually chosen hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "K2QuEbE1Clxq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "41.32766268072842\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "\"\"\"\n",
        "Manually choose the hyperparameters (learning rate and number of features) and train the model.\n",
        "Then compare the performance with random chosen hyperparameters and manually chosen hyperparameters.\n",
        "\"\"\"\n",
        "\n",
        "### START CODE HERE ###\n",
        "xnew = [20120.917, 32.000, 84.87882, 10.000, 24.98298, 121.54024]\n",
        "xnew = np.array(xnew)\n",
        "yn = np.dot(w,X[3][:6])+b\n",
        "print(yn)\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ0CviUnZui0"
      },
      "source": [
        "## Part 2: Logistic Regression\n",
        "\n",
        "### Problem Statement\n",
        "A Food grain industry requires an efficient classification system to help in sorting food grain species. You have to develop a Logistic Regression model for this purpose.\n",
        " Given various features of a rice grain such as area, perimeter, axis lengths etc. as input features, the task is to build a logistic regression model to predict the species of the food grain.\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "**For Even Roll Number Students**\n",
        "\n",
        "Dataset Filename: Rice_Classification.csv\n",
        "\n",
        "Attribute Information:\n",
        "+ Area: Returns the number of pixels within the boundaries of the rice grain.\n",
        "+ Perimeter: Calculates the circumference by calculating the distance between pixels around the boundaries of the rice grain.\n",
        "+ Major Axis Length: The longest line that can be drawn on the rice grain, i.e. the main axis distance, gives.\n",
        "+ Minor Axis Length: The shortest line that can be drawn on the rice grain, i.e. the small axis distance, gives.\n",
        "+ Eccentricity: It measures how round the ellipse, which has the same moments as the rice grain, is.\n",
        "+ Convex Area: Returns the pixel count of the smallest convex shell of the region formed by the rice grain.\n",
        "+ Extent: Returns the ratio of the region formed by the rice grain to the bounding box pixels.\n",
        "\n",
        "Target Variable: Class: Cammeo and Osmancik\n",
        "\n",
        "**For Odd Roll Number Students**\n",
        "\n",
        "Dataset Filename: Pumpkin_Seeds_Dataset.csv\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "+ Area: Represents the number of pixels within the boundaries of the pumpkin seed.\n",
        "\n",
        "+ Perimeter: Calculates the circumference by measuring the distance between pixels around the boundary of the pumpkin seed.\n",
        "\n",
        "+ Major Axis Length: The longest line that can be drawn on the pumpkin seed, representing the main axis distance.\n",
        "\n",
        "+ Minor Axis Length: The shortest line that can be drawn on the pumpkin seed, representing the minor axis distance.\n",
        "\n",
        "+ Convex Area: Returns the pixel count of the smallest convex shell that can contain the pumpkin seed.\n",
        "\n",
        "+ Equivalent Diameter: Diameter of a circle with the same area as the pumpkin seed.\n",
        "\n",
        "+ Eccentricity: This measures how round the ellipse, which has the same moments as the pumpkin seed has.\n",
        "\n",
        "+ Solidity: This is the ratio of the area of the pumpkin seed to the area of its convex hull. It measures the extent to which the shape is convex.\n",
        "\n",
        "+ Extent: Returns the ratio of the area of the pumpkin seed to the area of its bounding box.\n",
        "\n",
        "+ Roundness: Measure of how closely the shape of the pumpkin seed approaches that of a circle.\n",
        "\n",
        "+ Aspect Ratio: Ratio of the major axis length to the minor axis length.\n",
        "\n",
        "+ Compactness: Measure of the shape's compactness, which is the shape's deviation from being a perfect circle. In essence, compactness quantifies how efficiently an object's area is packed within its perimeter.\n",
        "\n",
        "Target Variable: Class: Çerçevelik and Ürgüp Sivrisi\n",
        "\n",
        "\n",
        "\n",
        "These are the following steps or functions that you have to complete to create and train the linear regression model:\n",
        "1. Reading the data\n",
        "2. Creating the sigmoid function\n",
        "2. Computing the loss function\n",
        "3. Computing the gradient of the loss\n",
        "4. Training the model using Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "T27XOBZrRZ4M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import logsumexp\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3luUk4L0WO_"
      },
      "source": [
        "### 2.1. Reading the data\n",
        "\n",
        "In the function ```load_data```, you have to read data from the file, store it in a dataframe and split the data from the dataframe into two numpy arrays X and y.\n",
        "\n",
        "**X** : data of the input features\n",
        "\n",
        "**y**  : data of the class labels\n",
        "\n",
        "The class labels in **y** should be replaced with '0' and '1', for corresponding classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-RV0JgEZi1h"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    This function loads the data into a pandas dataframe and converts it into X and y numpy arrays\n",
        "    y should be a binary numpy array with values 0 and 1, for 2 different classes\n",
        "    Args:\n",
        "        filepath: File path as a string\n",
        "    Returns:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "        y: Target variable of the shape (# of sample,) with values 0 and 1, for 2 different classes\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    df = pd.read_csv(filepath)\n",
        "    \n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X,y\n",
        "\n",
        "filepath = None\n",
        "### START CODE HERE ###\n",
        "## set the file path\n",
        "\n",
        "### END CODE HERE ###\n",
        "\n",
        "X, y = load_data(filepath)\n",
        "\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLGPG1YU-C47"
      },
      "source": [
        "We will not use all the features from X.\n",
        "\n",
        "For Even Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r between 4 and 7 (both inclusive) randomly. Use the first r features of the numpy array X.\n",
        "\n",
        "For Odd Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r between 8 and 12 (both inclusive) randomly. Use the first r features of the numpy array X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNsDb2Ae94s6"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def random_feature_selection(X):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "    Returns:\n",
        "        X_new: New input data of the shape (# of samples, r) containg only the first r features from X\n",
        "    \"\"\"\n",
        "\n",
        "    X_new = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X_new\n",
        "\n",
        "X = random_feature_selection(X)\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4u-qBe34wC9"
      },
      "source": [
        "We need to pre-process the data. We are using min-max scaler to scale the input data ($X$).\n",
        "\n",
        "After that, we split the data (```X``` and ```y```) into a training dataset (```X_train``` and ```y_train```) and test dataset (```X_test``` and ```y_test```)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTO4etePa0i1"
      },
      "outputs": [],
      "source": [
        "## Data scaling and train-test split\n",
        "\n",
        "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_index = int(X.shape[0] * (1 - test_size))\n",
        "\n",
        "    train_indices = indices[:split_index]\n",
        "    test_indices = indices[split_index:]\n",
        "\n",
        "    X_train = X[train_indices]\n",
        "    X_test = X[test_indices]\n",
        "    y_train = y[train_indices]\n",
        "    y_test = y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def min_max_scaler(X, feature_range=(0, 1)):\n",
        "    X_min = np.min(X, axis=0)\n",
        "    X_max = np.max(X, axis=0)\n",
        "\n",
        "    X_scaled = (X-X_min)/(X_max-X_min)\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "# Feature normalization\n",
        "X = min_max_scaler(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n",
        "print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8SbmjD945Ra"
      },
      "source": [
        "### 2.2. Creating the Sigmoid Function\n",
        "Recall that for logistic regression, the model is represented as\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
        "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
        "\n",
        "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "\n",
        "The function below returns the value of the sigmoid function for an input numpy array z. If the numpy array 'z' stores multiple numbers, we'd like to apply the sigmoid function to each value in the input array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G44B1vITbBpE"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z: A scalar or numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g: sigmoid(z)\n",
        "    \"\"\"\n",
        "    g = None\n",
        "    z = z.astype(float)\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "\n",
        "\n",
        "    ### END SOLUTION ###\n",
        "\n",
        "    return g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV-Ygf1R8HMl"
      },
      "source": [
        "### 2.3. Computing the loss Function\n",
        "\n",
        "Recall that for logistic regression, the cost function is of the form\n",
        "\n",
        "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
        "\n",
        "where\n",
        "* m is the number of training examples in the dataset\n",
        "\n",
        "\n",
        "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is -\n",
        "\n",
        "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
        "    \n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
        "\n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
        "\n",
        "Please complete the function loss_function that takes\n",
        "\n",
        " **X**  (input features)\n",
        "\n",
        " **y**  (class labels)\n",
        "\n",
        " **w**  (Parameters of the logistic regression model, (excluding the bias), a numpy array of the shape(1, number of features))\n",
        "\n",
        " **b**  (Bias value of the logistic regression model)\n",
        "\n",
        " You can use the Sigmoid function that you implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9RVguhHbEVk"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def loss_function(X, y, w, b):\n",
        " \"\"\"\n",
        " Computes the loss function for all the training examples\n",
        " Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b: Bias parameter (scalar) of the logistic regression model\n",
        "\n",
        "  Returns:\n",
        "        total_cost: The loss function value of using w and b as the parameters to fit the data points in X and y\n",
        "\n",
        " \"\"\"\n",
        " m, n = X.shape\n",
        "\n",
        " total_cost = 0\n",
        " ### START CODE HERE ###\n",
        "\n",
        " ### END CODE HERE ###\n",
        "\n",
        " return total_cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9YmUPnsASYE"
      },
      "source": [
        "### 2.4. Computing the Gradient of the Loss\n",
        "\n",
        "Recall that the gradient descent algorithm is:\n",
        "\n",
        "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
        "\n",
        "where, parameters $b$, $w_j$ are all updated simultaniously\n",
        "\n",
        "In this step, you are required to complete the `compute_gradient_logistic_regression` function to compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
        "$$\n",
        "* m is the number of training examples in the dataset\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label\n",
        "\n",
        "You can use the sigmoid function that you implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWxbewKkbIB8"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def compute_gradient_logistic_regression(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient values of the loss function\n",
        "    Args:\n",
        "       X: Input data of the shape (# of training samples, # of input features)\n",
        "       y: Target variable of the shape (# of training sample,)\n",
        "       w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "       b: Bias parameter of the logistic regression model of the shape (1,1) or a scaler\n",
        "    Returns:\n",
        "       dL_dw : The gradient of the cost w.r.t. the parameters w with shape same as w\n",
        "       dL_db : The gradient of the cost w.r.t. the parameter b with shape same as b\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "    dj_db = 0\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    return dj_db, dj_dw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZMYX655MRFX"
      },
      "source": [
        "### 2.5. Training the model using Batch Gradient Descent\n",
        "\n",
        "Please complete the batch gradient descent algorithm for logistic regression to train and learn the parameters of the logistic regression model. You have to use ```loss_function``` and ```compute_gradient_logistic regression``` functions that you have implemented earlier in this assignment.\n",
        "\n",
        "In this ```batch_gradient_descent_logistic_regression``` function, you have to compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n",
        "\n",
        "+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
        "\n",
        "+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "This function takes   ```X```    (input features),  ```y```  (class labels),  ```w_in```  (intial values of parameters(excluding bias)),  ```b_in```  (initial value for bias),  ```num_iters```   (number of iterations of training) as input.\n",
        "\n",
        "Additionally, you have compute the loss function values in every iteration and store it in the list variable ```loss_hist``` and print the loss value after every 100 iterations during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOi07RVNbL_7"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def batch_gradient_descent_logistic_regression(X, y, w_in, b_in, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Batch gradient descent to learn the parameters (w and b) of the linear regression model and to print loss values\n",
        "    every 100 iterations\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w_in: Initial parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b_in: Initial bias parameter (scalar) of the logistic regression model\n",
        "        alpha: Learning rate\n",
        "        num_iters: number of iterations\n",
        "    Returns\n",
        "        w: Updated values of parameters of the model after training\n",
        "        b: Updated bias of the model after training\n",
        "        loss_hist: List of loss values for every iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # list to store the loss values for every iterations\n",
        "    loss_hist = []\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return w_in, b_in, loss_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yeLdzT9-5Wc"
      },
      "source": [
        "Now you have to intialize the model parameters ($w$ and $b$) and learning rate (```alpha```). The learning rate ```alpha``` has to be randomly initialized between 0.01 and 0.09. To randomly initialize the learning rate, you have to first set the last two digits of your roll number as the random seed using ```random.seed()``` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIBltyJh-Tjk"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "## set the last two digits of your roll number as the random seed\n",
        "random_seed = None\n",
        "### START CODE HERE ###\n",
        "random_seed = 15\n",
        "### END CODE HERE ###\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    This function randomly initializes the model parameters (w and b) and the hyperparameter alpha\n",
        "    Initial w and b should be randomly sampled from a normal distribution with mean 0\n",
        "    alpha should be randomly initialized between 0.01 and 0.09 by using last two digits of your roll number as the random seed\n",
        "    Args:\n",
        "        None\n",
        "    Returns:\n",
        "        initial_w: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        initial_b: Initial bias parameter (scalar) of the linear regression model\n",
        "        alpha: Learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    initial_w = None\n",
        "    initial_b = None\n",
        "    alpha = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return initial_w,initial_b,alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT5CbQ8dZS_J"
      },
      "source": [
        "The following cell runs the batch gradient algorithm for\n",
        "```num_iterations=1000``` to train the logistic regression model. You can change the number of iterations to check any improvements in the performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFTZi7pkbOdp"
      },
      "outputs": [],
      "source": [
        "# initialize the parameters (w an b) randomly\n",
        "initial_w, initial_b, alpha = initialize_parameters()\n",
        "num_iterations = 1000\n",
        "\n",
        "w, b, loss_hist = batch_gradient_descent_logistic_regression(X_train ,y_train, initial_w, initial_b, alpha, num_iterations)\n",
        "print(\"optimized parameter values w:\", w)\n",
        "print(\"optimized parameter value b:\", b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYP0-YUyY50x"
      },
      "source": [
        "### 2.6. Final Train and Test Accuracy\n",
        "After the logistic regression model is trained, we will predict the class labels for the training set and test set and we will compute the accuracy.\n",
        "\n",
        "Please complete the `predict` function to produce `1` or `0` predictions given a dataset and a learned parameter vector $w$ and $b$.\n",
        "- First you need to compute the prediction from the model $f(x^{(i)}) = g(w \\cdot x^{(i)})$ for every example\n",
        "\n",
        "- We interpret the output of the model ($f(x^{(i)})$) as the probability that $y^{(i)}=1$ given $x^{(i)}$ and parameterized by $w$.\n",
        "- Therefore, to get a final prediction ($y^{(i)}=0$ or $y^{(i)}=1$) from the logistic regression model, you can use the following heuristic -\n",
        "\n",
        "  if $f(x^{(i)}) >= 0.5$, predict $y^{(i)}=1$\n",
        "  \n",
        "  if $f(x^{(i)}) < 0.5$, predict $y^{(i)}=0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZDUV2iwbQ_R"
      },
      "outputs": [],
      "source": [
        "def predict(X, w, b):\n",
        "      \"\"\"\n",
        "      Predict whether the label is 0 or 1 using learned logistic regression parameters (w,b)\n",
        "\n",
        "      Args:\n",
        "        X: Input data of shape (number of sample, number of features)\n",
        "        w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b: Bias parameter of the logistic regression model\n",
        "\n",
        "      Returns:\n",
        "        p: Predictions for X using a threshold at 0.5\n",
        "      \"\"\"\n",
        "      m, n = X.shape\n",
        "      p = np.zeros(m)\n",
        "      ### START CODE HERE###\n",
        "\n",
        "\n",
        "      ### END CODE HERE ###\n",
        "      return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsrATkIy2KVy"
      },
      "source": [
        "Now let's use this to compute the accuracy on the training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMmh6WdFbU9Y"
      },
      "outputs": [],
      "source": [
        "p_train = predict(X_train, w,b)\n",
        "print('Train Accuracy: %f'%(np.mean(p_train == y_train) * 100))\n",
        "p_test = predict(X_test, w,b)\n",
        "print('Test Accuracy: %f'%(np.mean(p_test == y_test) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0VqjFy9ZK7D"
      },
      "source": [
        "Now, we plot the loss function values for every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkmP3XHMTW74"
      },
      "outputs": [],
      "source": [
        "# PLotting the loss values for every training iterations\n",
        "\n",
        "loss_plot = [loss_hist[i][0][0] for i in range(len(loss_hist))]\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss function\")\n",
        "plt.plot(loss_plot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgKAU9RBEUlL"
      },
      "source": [
        "### 2.7 Experimenting with different values of the Hyperparemeters\n",
        "\n",
        "Previously, we randomly sampled the learning rate and the number of features to train the model. Now, you have to manually choose the number of features and the learning rate. Then, you have to train the model again on the manually choosen hyperparameters (number of features and learning rate). In the next cell, you have to manually choose the hyperparameters and write the code to train the model.\n",
        "\n",
        "After the model is trained, you have to compare the performance of the model with random chosen hyperparameters and the model **with** manually chosen hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMcc3cuvIbD_"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "\"\"\"\n",
        "Manually choose the hyperparameters (learning rate and number of features) and train the model.\n",
        "Then compare the performance with random chosen hyperparameters and manually chosen hyperparameters.\n",
        "\"\"\"\n",
        "\n",
        "### START CODE HERE ###\n",
        "\n",
        "### END CODE HERE ###"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
