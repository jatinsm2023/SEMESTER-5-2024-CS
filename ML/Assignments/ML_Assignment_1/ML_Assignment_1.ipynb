{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emjAQ_34SozP",
        "tags": []
      },
      "source": [
        "# Programming Assignment 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKkKCHwsSozQ"
      },
      "source": [
        "In this programming assignment, you will implement a linear regression model and a logistic regression model.\n",
        "\n",
        "In Part 1, you have to implement a linear regression model to predict the price of a house based on various input features.\n",
        "\n",
        "In Part 2, you have to implement a logistic regression model to predict the species of a grain using various morphological features.\n",
        "\n",
        "The assignment zip file (ML_Assignment_1.zip) contains 4 datasets which will be used in this assignment.\n",
        "\n",
        "You have to write your code in this jupyter notebook and submit the solved jupyter notebook with the file name \\<Roll_No\\>_A1.ipynb for evaluation. You have to enter your code only in those cells which are marked as ```## CODE REQUIRED ##```, and you have to write your code only between ```### START CODE HERE ###``` and ```### END CODE HERE ###``` comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHPf8pHpSozR"
      },
      "source": [
        "## Part 1: Linear Regression\n",
        "\n",
        "### Problem Statement  \n",
        "A real estate company is building a machine learning model to determine the price of a house. The model will take various information regarding a house as input features and predict the price per unit area. They decided to use the linear regression as the machine learning model. Your task is to help the company to build the model.\n",
        "Given various features of a house, you will create a linear regression model to predict the price of the house.\n",
        "\n",
        "### Data Description\n",
        "\n",
        "**For Even Roll Number Students:**\n",
        "\n",
        "Dataset Filename: Taiwan_House.csv\n",
        "\n",
        "Attributes Information:\n",
        "1. Transaction date\n",
        "2. House age\n",
        "3. Distance to the nearest MRT station\n",
        "4. Number of convenience stores\n",
        "5. Latitude\n",
        "6. Longitude\n",
        "\n",
        "Target variable: house price of unit area\n",
        "\n",
        "**For Odd Roll Number Students:**\n",
        "\n",
        "Dataset Filename: Boston_House.csv\n",
        "\n",
        "Attributes Information:\n",
        "1. CRIM: per capita crime rate by town\n",
        "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "3. INDUS: proportion of non-­retail business acres per town\n",
        "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "5. NOX: nitric oxides concentration (parts per 10 million)\n",
        "6. RM: average number of rooms per dwelling\n",
        "7. AGE: proportion of owner­occupied units built prior to 1940\n",
        "8. DIS: weighted distances to five Boston employment centres\n",
        "9. RAD: index of accessibility to radial highways\n",
        "10. TAX: full-­value property-­tax rate per $10,000\n",
        "\n",
        "11. PTRATIO: pupil-­teacher ratio by town\n",
        "12. B: 1000(Bk ­- 0.63)^2 where Bk is the proportion of blacks by town\n",
        "13. LSTAT: % lower status of the population\n",
        "\n",
        "Target Variable: MEDV: Median value of owner-­occupied homes in $1000's\n",
        "\n",
        "\n",
        "These are the following steps or functions that you have to complete to create and train the linear regression model:\n",
        "1. Reading the data\n",
        "2. Computing the loss function\n",
        "3. Computing the gradient of the loss\n",
        "4. Training the model using Batch Gradient Descent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "rJDNL8y8SozR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import math\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ExYIXiSozS"
      },
      "source": [
        "### 1.1. Reading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgnxuEDkSozS"
      },
      "source": [
        "In the following function ```load_data```, you have to read the data from the file and store the data into a pandas dataframe. Then you have to create two numpy arrays $X$ and $y$ from the dataframe:\n",
        "\n",
        "+ $X$: Input data of the shape (number of samples, number of input features)\n",
        "+ $y$: Target variable of the shape (number of samples,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 604,
      "metadata": {
        "id": "DEal5SFmSozS",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X:  (414, 6) Shape of y:  (414,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    This function loads the data into a pandas dataframe and coverts it into X and y numpy arrays\n",
        "\n",
        "    Args:\n",
        "        filepath: File path as a string\n",
        "    Returns:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "        y: Target variable of the shape (# of sample,)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    df = pd.read_csv(filepath)\n",
        "    \n",
        "    x_attr = df[['transaction date','house age','distance to the nearest MRT station','number of convenience stores','latitude','longitude']]\n",
        "    X = x_attr.to_numpy()\n",
        "    y_attr = df['house price of unit area']\n",
        "    y = y_attr.to_numpy()\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X,y\n",
        "\n",
        "filepath = None\n",
        "### START CODE HERE ###\n",
        "## set the file path\n",
        "filepath=\"Taiwan_House.csv\"\n",
        "### END CODE HERE ###\n",
        "X, y = load_data(filepath)\n",
        "\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se3Qrch5PoIO"
      },
      "source": [
        "We will not use all the features from ```X```.\n",
        "\n",
        "**For Even Roll Number Students:**\n",
        "Set the last two digits of your roll number as the random seed and pick a number ``r`` between 3 and 6 (both inclusive) randomly. Use the first ```r``` features of the numpy array ```X```.\n",
        "\n",
        "**For Odd Roll Number Students:**\n",
        "Set the last two digits of your roll number as the random seed and pick a number ``r`` between 9 and 13 (both inclusive) randomly. Use the first ```r``` features of the numpy array ```X```.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 605,
      "metadata": {
        "id": "mnRd0p8LtRJ5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X:  (414, 5) Shape of y:  (414,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def random_feature_selection(X):\n",
        "    \"\"\"\n",
        "    For Even Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r\n",
        "    between 3 and 6 randomly. Use the first ```r``` features of the numpy array X.\n",
        "    For Odd Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r\n",
        "    between 9 and 13 randomly. Use the first ```r``` features of the numpy array X.\n",
        "    Args:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "    Returns:\n",
        "        X_new: New input data of the shape (# of samples, r) containg only the first r features from X\n",
        "    \"\"\"\n",
        "\n",
        "    X_new = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    np.random.seed(32)\n",
        "    r = random.randint(3,6)\n",
        "\n",
        "    X_new = X[:, :r]\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X_new\n",
        "\n",
        "X = random_feature_selection(X)\n",
        "\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAc8UixtqLS"
      },
      "source": [
        "We need to pre-process the data. We are using min-max scaler to scale the input data ($X$).\n",
        "\n",
        "After that, we split the data (```X``` and ```y```) into a training dataset (```X_train``` and ```y_train```) and test dataset (```X_test``` and ```y_test```)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 606,
      "metadata": {
        "id": "LLmD1I3-SozT",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train:  (310, 5) Shape of y_train:  (310,)\n",
            "Shape of X_test:  (104, 5) Shape of y_test:  (104,)\n"
          ]
        }
      ],
      "source": [
        "## Data scaling and train-test split\n",
        "\n",
        "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_index = int(X.shape[0] * (1 - test_size))\n",
        "\n",
        "    train_indices = indices[:split_index]\n",
        "    test_indices = indices[split_index:]\n",
        "\n",
        "    X_train = X[train_indices]\n",
        "    X_test = X[test_indices]\n",
        "    y_train = y[train_indices]\n",
        "    y_test = y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def min_max_scaler(X, feature_range=(0, 1)):\n",
        "    X_min = np.min(X, axis=0)\n",
        "    X_max = np.max(X, axis=0)\n",
        "    \n",
        "    X_scaled = (X-X_min)/(X_max-X_min)\n",
        "   \n",
        "    return X_scaled\n",
        "\n",
        "\n",
        "X = min_max_scaler(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n",
        "print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrFEUV2-SozU"
      },
      "source": [
        "### 1.2. Computing the Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1BC0TPCSozU"
      },
      "source": [
        "In linear regression, the model parameters are:\n",
        "\n",
        "+ $w$: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "\n",
        "+ $b$: Bias parameter (scalar) of the linear regression model\n",
        "\n",
        "Both $w$ and $b$ are numpy arrays.\n",
        "\n",
        "Given the model parameters $w$ and $b$, the prediction for an input sample $X^i$ is:\n",
        "$$h_{w,b}(X^i) = w \\cdot X^i + b$$\n",
        "where $X^i$ is the $i^{th}$ training sample with shape (number of features,1)\n",
        "\n",
        "For linear regression, you have to implement and compute Mean Squarred Error loss fucntion:\n",
        "$$ L_{w,b}(X) = \\sum_{i=1}^{m}(y^i - h_{w,b}(X^i))^2 $$\n",
        "where $y^i$ is the true target value for the $i^{th}$ sample and $h_{w,b}(X^i)$ is the predicted value for the $i^{th}$ sample using the parameters $w$ and $b$.\n",
        "\n",
        "$w$ is the list of parameters excluding the bias and $b$ is the bias term."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 607,
      "metadata": {
        "id": "t6GqZMlYSozV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def loss_function(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function for linear regression.\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b: Bias parameter (scalar) of the linear regression model\n",
        "\n",
        "    Returns\n",
        "        loss: The loss function value of using w and b as the parameters to fit the data points in X and y\n",
        "    \"\"\"\n",
        "    # number of training examples\n",
        "    m = X.shape[0]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    loss = 0\n",
        "    y_pred = np.dot(X,w.T)+b\n",
        "    for i in range(m):\n",
        "        loss += (y_pred[i]-y[i])**2\n",
        "    \n",
        "    loss/=2*m\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0h7jhxGSozV"
      },
      "source": [
        "### 1.3. Comptuing the Gradient of the Loss\n",
        "\n",
        "In this following function ```compute_gradient```, you have to compute the gradients $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$ of the loss $L$ w.r.t. $w$ and $b$. More specifically, you have to iterate over every training example and compute the gradients of the loss for that training example. Finally, aggregate the gradient values for all the training examples and take the average. The gradients can be computed as:\n",
        "$$\\frac{\\partial L}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^m (h_{w,b}(X^i)-y^i)X^i$$\n",
        "\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{w,b}(X^i)-y^i)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 608,
      "metadata": {
        "id": "do-IiCTZSozV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient values\n",
        "    Args:\n",
        "       X: Input data of the shape (# of training samples, # of input features)\n",
        "       y: Target variable of the shape (# of training sample,)\n",
        "       w: Parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "       b: Bias parameter of the linear regression model of the shape (1,1) or a scaler\n",
        "    Returns:\n",
        "       dL_dw : The gradient of the cost w.r.t. the parameters w with shape same as w\n",
        "       dL_db : The gradient of the cost w.r.t. the parameter b with shape same as b\n",
        "    \"\"\"\n",
        "\n",
        "   # Number of training examples\n",
        "    m = X.shape[0]\n",
        "\n",
        "    predictions = np.dot(X, w.T) + b\n",
        "\n",
        "    errors = predictions - y.reshape(-1, 1)\n",
        "\n",
        "    dL_dw = np.dot(errors.T, X) / m\n",
        "    dL_db = np.sum(errors) / m\n",
        "    \n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    return dL_dw, dL_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYL1jWaZSozV"
      },
      "source": [
        "### 1.4. Training the Model using Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vwyRzWqSozV"
      },
      "source": [
        "Finally, you have to implement the batch gradient descent algorithm to train and learn the parameters of the linear regression model. You have to use ```loss_function``` and ```compute_gradient``` functions that you have implemented earlier in this assignment.\n",
        "\n",
        "In this ```batch_gradient_descent``` function, you have to compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n",
        "\n",
        "+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
        "\n",
        "+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "Additionally, you have compute the loss function values in every iteration and store it in the list variable ```loss_hist``` and print the loss value after every 100 iterations during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 609,
      "metadata": {
        "id": "UHKldb3hSozV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def batch_gradient_descent(X, y, w_initial, b_initial, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Batch gradient descent to learn the parameters (w and b) of the linear regression model and to print loss values\n",
        "    every 100 iterations\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w_initial: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b_initial: Initial bias parameter (scalar) of the linear regression model\n",
        "        alpha: Learning rate\n",
        "        num_iters: number of iterations\n",
        "    Returns\n",
        "        w: Updated values of parameters of the model after training\n",
        "        b: Updated bias of the model after training\n",
        "        loss_hist: List of loss values for every iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # number of training examples\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # to store loss values for every iteation as a list and print loss value after every 100 iterations\n",
        "    loss_hist = []\n",
        "\n",
        "    # Initialize parameters\n",
        "    w = copy.deepcopy(w_initial) ## deepcopy is used so that the updates do not change the initial variable values\n",
        "    b = b_initial\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    for i in range(num_iters):\n",
        "        dw,db = compute_gradient(X,y,w,b)\n",
        "\n",
        "        w-=alpha*dw\n",
        "        b-=alpha*db\n",
        "\n",
        "        loss = loss_function(X,y,w,b)\n",
        "        loss_hist.append(loss)\n",
        "\n",
        "        if i%100==0:\n",
        "            print(f\"Iteration {i}: {loss}\")        \n",
        "\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return w, b, loss_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0h3DUMwNocp"
      },
      "source": [
        "Now you have to intialize the model parameters ($w$ and $b$) and learning rate (```alpha```). The learning rate ```alpha``` has to be randomly initialized between 0.0001 and 0.001. For the learning rate, you have to first set the last two digits of your roll number as the random seed using ```random.seed()``` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 610,
      "metadata": {
        "id": "Xd8QFReNDyGs"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "## set the last two digits of your roll number as the random seed\n",
        "random_seed = None\n",
        "### START CODE HERE ###\n",
        "random_seed = 32\n",
        "### END CODE HERE ###\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    This function randomly initializes the model parameters (w and b) and the hyperparameter alpha\n",
        "    Initial w and b should be randomly sampled from a normal distribution with mean 0\n",
        "    alpha should be randomly initialized between 0.0001 and 0.001 by using last two digits of your roll number as the random seed\n",
        "    Args:\n",
        "        None\n",
        "    Returns:\n",
        "        initial_w: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        initial_b: Initial bias parameter (scalar) of the linear regression model\n",
        "        alpha: Learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    initial_w = None\n",
        "    initial_b = None\n",
        "    alpha = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    np.random.seed(seed=random_seed)\n",
        "    initial_w = np.random.normal(0,1,size=(1,X_train.shape[1]))\n",
        "\n",
        "    initial_b=np.random.normal(0,1)\n",
        "    alpha = np.random.uniform(0.0001,0.001)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return initial_w,initial_b,alpha\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7nt5VdORUEu"
      },
      "source": [
        "In the next cell, the model is trained using batch gradient descent algorithm for ```num_iters=10000``` iterations. You can change the number of iterations to check any improvements in the performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 611,
      "metadata": {
        "id": "9rAYubtASozV",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: [748.38007615]\n",
            "Iteration 100: [702.32162211]\n",
            "Iteration 200: [659.39862833]\n",
            "Iteration 300: [619.3971089]\n",
            "Iteration 400: [582.11768369]\n",
            "Iteration 500: [547.37458137]\n",
            "Iteration 600: [514.99471054]\n",
            "Iteration 700: [484.81679423]\n",
            "Iteration 800: [456.69056354]\n",
            "Iteration 900: [430.47600622]\n",
            "Iteration 1000: [406.04266661]\n",
            "Iteration 1100: [383.26899337]\n",
            "Iteration 1200: [362.04173165]\n",
            "Iteration 1300: [342.25535687]\n",
            "Iteration 1400: [323.81154702]\n",
            "Iteration 1500: [306.61869108]\n",
            "Iteration 1600: [290.59143096]\n",
            "Iteration 1700: [275.65023467]\n",
            "Iteration 1800: [261.72099865]\n",
            "Iteration 1900: [248.73467729]\n",
            "Iteration 2000: [236.62693766]\n",
            "Iteration 2100: [225.33783785]\n",
            "Iteration 2200: [214.81152724]\n",
            "Iteration 2300: [204.99596728]\n",
            "Iteration 2400: [195.84267127]\n",
            "Iteration 2500: [187.30646191]\n",
            "Iteration 2600: [179.34524545]\n",
            "Iteration 2700: [171.91980119]\n",
            "Iteration 2800: [164.9935854]\n",
            "Iteration 2900: [158.53254858]\n",
            "Iteration 3000: [152.50496523]\n",
            "Iteration 3100: [146.88127519]\n",
            "Iteration 3200: [141.6339359]\n",
            "Iteration 3300: [136.73728464]\n",
            "Iteration 3400: [132.16741023]\n",
            "Iteration 3500: [127.90203349]\n",
            "Iteration 3600: [123.92039585]\n",
            "Iteration 3700: [120.20315551]\n",
            "Iteration 3800: [116.73229081]\n",
            "Iteration 3900: [113.49101007]\n",
            "Iteration 4000: [110.46366766]\n",
            "Iteration 4100: [107.63568577]\n",
            "Iteration 4200: [104.99348152]\n",
            "Iteration 4300: [102.52439905]\n",
            "Iteration 4400: [100.21664626]\n",
            "Iteration 4500: [98.05923585]\n",
            "Iteration 4600: [96.04193035]\n",
            "Iteration 4700: [94.15519101]\n",
            "Iteration 4800: [92.39013004]\n",
            "Iteration 4900: [90.73846623]\n",
            "Iteration 5000: [89.19248351]\n",
            "Iteration 5100: [87.74499239]\n",
            "Iteration 5200: [86.38929404]\n",
            "Iteration 5300: [85.11914675]\n",
            "Iteration 5400: [83.92873479]\n",
            "Iteration 5500: [82.81263927]\n",
            "Iteration 5600: [81.76581111]\n",
            "Iteration 5700: [80.78354578]\n",
            "Iteration 5800: [79.86145974]\n",
            "Iteration 5900: [78.99546861]\n",
            "Iteration 6000: [78.1817667]\n",
            "Iteration 6100: [77.41680798]\n",
            "Iteration 6200: [76.69728839]\n",
            "Iteration 6300: [76.02012929]\n",
            "Iteration 6400: [75.38246213]\n",
            "Iteration 6500: [74.78161403]\n",
            "Iteration 6600: [74.2150945]\n",
            "Iteration 6700: [73.68058294]\n",
            "Iteration 6800: [73.17591707]\n",
            "Iteration 6900: [72.69908212]\n",
            "Iteration 7000: [72.24820074]\n",
            "Iteration 7100: [71.82152367]\n",
            "Iteration 7200: [71.41742095]\n",
            "Iteration 7300: [71.03437379]\n",
            "Iteration 7400: [70.67096699]\n",
            "Iteration 7500: [70.32588188]\n",
            "Iteration 7600: [69.99788969]\n",
            "Iteration 7700: [69.68584546]\n",
            "Iteration 7800: [69.38868231]\n",
            "Iteration 7900: [69.10540608]\n",
            "Iteration 8000: [68.8350904]\n",
            "Iteration 8100: [68.57687206]\n",
            "Iteration 8200: [68.32994668]\n",
            "Iteration 8300: [68.09356472]\n",
            "Iteration 8400: [67.86702769]\n",
            "Iteration 8500: [67.64968474]\n",
            "Iteration 8600: [67.44092935]\n",
            "Iteration 8700: [67.24019631]\n",
            "Iteration 8800: [67.04695894]\n",
            "Iteration 8900: [66.86072642]\n",
            "Iteration 9000: [66.68104134]\n",
            "Iteration 9100: [66.50747747]\n",
            "Iteration 9200: [66.33963756]\n",
            "Iteration 9300: [66.17715143]\n",
            "Iteration 9400: [66.01967408]\n",
            "Iteration 9500: [65.86688398]\n",
            "Iteration 9600: [65.71848148]\n",
            "Iteration 9700: [65.57418731]\n",
            "Iteration 9800: [65.43374118]\n",
            "Iteration 9900: [65.29690051]\n",
            "Updated w:  [[ 8.8579405   6.74222968  0.99243713 10.6578504  10.80950026]]\n",
            "Updated b:  19.96781681933579\n"
          ]
        }
      ],
      "source": [
        "# initialize the parameters and hyperparameter\n",
        "initial_w, initial_b, alpha = initialize_parameters()\n",
        "\n",
        "# number of iterations\n",
        "num_iters = 10000\n",
        "\n",
        "\n",
        "w,b,loss_hist = batch_gradient_descent(X_train ,y_train, initial_w, initial_b, alpha, num_iters)\n",
        "print(\"Updated w: \",w)\n",
        "print(\"Updated b: \",b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sluek2LkSozW"
      },
      "source": [
        "### 1.5. Final Train Error and Test Error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH9oHC10fAlV"
      },
      "source": [
        "After the linear regression model is trained, we will compute the final train error and test error for the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 612,
      "metadata": {
        "id": "UIXb7sSvSozW",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Error:  [65.16475777] , Test Error:  [101.48269921]\n"
          ]
        }
      ],
      "source": [
        "## Train and Test error computation\n",
        "\n",
        "train_error = loss_function(X_train,y_train,w,b)\n",
        "test_error = loss_function(X_test,y_test,w,b)\n",
        "print(\"Train Error: \",train_error, \", Test Error: \",test_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eximd12PbAgC"
      },
      "source": [
        "### 1.6. Plotting the loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meDwLCCIfOBo"
      },
      "source": [
        "We will plot the loss function values for every training iteration. If the model is trained properly, you will see that the loss function reduces as the training progesses and it converges at some point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 613,
      "metadata": {
        "id": "DW4bue0oSozW"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# PLotting the loss values for every training iterations\n",
        "\n",
        "loss_plot = [loss_hist[i] for i in range(len(loss_hist))]\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss function\")\n",
        "plt.plot(loss_plot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAVgq4HWCmDX"
      },
      "source": [
        "### 1.7 Experimenting with different values of the Hyperparemeters\n",
        "\n",
        "Previously, we randomly sampled the learning rate and the number of features to train the model. Now, you have to manually choose the number of features and the learning rate. Then, you have to train the model again on the manually choosen hyperparameters (number of features and learning rate). In the next cell, you have to manually choose the hyperparameters and write the code to train the model.\n",
        "\n",
        "After the model is trained, you have to compare the performance of the model with random chosen hyperparameters and manually chosen hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 619,
      "metadata": {
        "id": "K2QuEbE1Clxq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: [513.33523027]\n",
            "Iteration 100: [38.82002893]\n",
            "Iteration 200: [34.82303761]\n",
            "Iteration 300: [33.78925331]\n",
            "Iteration 400: [33.39530615]\n",
            "Iteration 500: [33.18652511]\n",
            "Iteration 600: [33.04848797]\n",
            "Iteration 700: [32.94505361]\n",
            "Iteration 800: [32.86209976]\n",
            "Iteration 900: [32.79289475]\n",
            "Iteration 1000: [32.73366114]\n",
            "Iteration 1100: [32.6820212]\n",
            "Iteration 1200: [32.63636302]\n",
            "Iteration 1300: [32.59554007]\n",
            "Iteration 1400: [32.55870928]\n",
            "Iteration 1500: [32.52523488]\n",
            "Iteration 1600: [32.49462739]\n",
            "Iteration 1700: [32.46650308]\n",
            "Iteration 1800: [32.44055611]\n",
            "Iteration 1900: [32.41653891]\n",
            "Iteration 2000: [32.39424808]\n",
            "Iteration 2100: [32.37351415]\n",
            "Iteration 2200: [32.35419397]\n",
            "Iteration 2300: [32.33616506]\n",
            "Iteration 2400: [32.31932134]\n",
            "Iteration 2500: [32.30356992]\n",
            "Iteration 2600: [32.28882864]\n",
            "Iteration 2700: [32.27502412]\n",
            "Iteration 2800: [32.26209034]\n",
            "Iteration 2900: [32.24996747]\n",
            "Iteration 3000: [32.23860093]\n",
            "Iteration 3100: [32.22794075]\n",
            "Iteration 3200: [32.21794089]\n",
            "Iteration 3300: [32.20855884]\n",
            "Iteration 3400: [32.19975523]\n",
            "Iteration 3500: [32.19149348]\n",
            "Iteration 3600: [32.18373953]\n",
            "Iteration 3700: [32.17646166]\n",
            "Iteration 3800: [32.16963023]\n",
            "Iteration 3900: [32.16321757]\n",
            "Iteration 4000: [32.15719778]\n",
            "Iteration 4100: [32.15154662]\n",
            "Iteration 4200: [32.14624138]\n",
            "Iteration 4300: [32.1412608]\n",
            "Iteration 4400: [32.13658493]\n",
            "Iteration 4500: [32.13219507]\n",
            "Iteration 4600: [32.12807369]\n",
            "Iteration 4700: [32.12420435]\n",
            "Iteration 4800: [32.12057159]\n",
            "Iteration 4900: [32.11716093]\n",
            "Iteration 5000: [32.1139588]\n",
            "Iteration 5100: [32.11095241]\n",
            "Iteration 5200: [32.10812981]\n",
            "Iteration 5300: [32.10547975]\n",
            "Iteration 5400: [32.10299168]\n",
            "Iteration 5500: [32.10065569]\n",
            "Iteration 5600: [32.09846249]\n",
            "Iteration 5700: [32.09640334]\n",
            "Iteration 5800: [32.09447005]\n",
            "Iteration 5900: [32.09265493]\n",
            "Iteration 6000: [32.09095075]\n",
            "Iteration 6100: [32.08935073]\n",
            "Iteration 6200: [32.08784851]\n",
            "Iteration 6300: [32.0864381]\n",
            "Iteration 6400: [32.0851139]\n",
            "Iteration 6500: [32.08387063]\n",
            "Iteration 6600: [32.08270336]\n",
            "Iteration 6700: [32.08160742]\n",
            "Iteration 6800: [32.08057847]\n",
            "Iteration 6900: [32.07961241]\n",
            "Iteration 7000: [32.07870539]\n",
            "Iteration 7100: [32.07785381]\n",
            "Iteration 7200: [32.07705428]\n",
            "Iteration 7300: [32.07630362]\n",
            "Iteration 7400: [32.07559884]\n",
            "Iteration 7500: [32.07493713]\n",
            "Iteration 7600: [32.07431586]\n",
            "Iteration 7700: [32.07373257]\n",
            "Iteration 7800: [32.07318493]\n",
            "Iteration 7900: [32.07267076]\n",
            "Iteration 8000: [32.07218801]\n",
            "Iteration 8100: [32.07173477]\n",
            "Iteration 8200: [32.07130924]\n",
            "Iteration 8300: [32.07090971]\n",
            "Iteration 8400: [32.0705346]\n",
            "Iteration 8500: [32.07018241]\n",
            "Iteration 8600: [32.06985176]\n",
            "Iteration 8700: [32.06954131]\n",
            "Iteration 8800: [32.06924983]\n",
            "Iteration 8900: [32.06897618]\n",
            "Iteration 9000: [32.06871924]\n",
            "Iteration 9100: [32.06847801]\n",
            "Iteration 9200: [32.06825153]\n",
            "Iteration 9300: [32.06803888]\n",
            "Iteration 9400: [32.06783924]\n",
            "Iteration 9500: [32.06765179]\n",
            "Iteration 9600: [32.06747581]\n",
            "Iteration 9700: [32.06731057]\n",
            "Iteration 9800: [32.06715544]\n",
            "Iteration 9900: [32.06700979]\n",
            "Train Error:  [32.06687437] , Test Error:  [58.47507835] alpha : 0.1\n",
            "Iteration 0: [286.11733632]\n",
            "Iteration 100: [34.79383843]\n",
            "Iteration 200: [33.39098175]\n",
            "Iteration 300: [33.04682153]\n",
            "Iteration 400: [32.86107737]\n",
            "Iteration 500: [32.7329221]\n",
            "Iteration 600: [32.63578819]\n",
            "Iteration 700: [32.5582418]\n",
            "Iteration 800: [32.49423536]\n",
            "Iteration 900: [32.44022008]\n",
            "Iteration 1000: [32.39395555]\n",
            "Iteration 1100: [32.35393653]\n",
            "Iteration 1200: [32.31909307]\n",
            "Iteration 1300: [32.28862521]\n",
            "Iteration 1400: [32.26190845]\n",
            "Iteration 1500: [32.23843796]\n",
            "Iteration 1600: [32.21779467]\n",
            "Iteration 1700: [32.19962395]\n",
            "Iteration 1800: [32.18362161]\n",
            "Iteration 1900: [32.16952431]\n",
            "Iteration 2000: [32.15710264]\n",
            "Iteration 2100: [32.14615593]\n",
            "Iteration 2200: [32.13650819]\n",
            "Iteration 2300: [32.12800481]\n",
            "Iteration 2400: [32.12050976]\n",
            "Iteration 2500: [32.11390333]\n",
            "Iteration 2600: [32.10808006]\n",
            "Iteration 2700: [32.10294706]\n",
            "Iteration 2800: [32.09842249]\n",
            "Iteration 2900: [32.0944342]\n",
            "Iteration 3000: [32.09091863]\n",
            "Iteration 3100: [32.08781974]\n",
            "Iteration 3200: [32.08508813]\n",
            "Iteration 3300: [32.08268029]\n",
            "Iteration 3400: [32.08055782]\n",
            "Iteration 3500: [32.07868691]\n",
            "Iteration 3600: [32.07703775]\n",
            "Iteration 3700: [32.07558405]\n",
            "Iteration 3800: [32.07430264]\n",
            "Iteration 3900: [32.0731731]\n",
            "Iteration 4000: [32.07217744]\n",
            "Iteration 4100: [32.07129979]\n",
            "Iteration 4200: [32.07052615]\n",
            "Iteration 4300: [32.06984421]\n",
            "Iteration 4400: [32.0692431]\n",
            "Iteration 4500: [32.06871322]\n",
            "Iteration 4600: [32.06824615]\n",
            "Iteration 4700: [32.06783444]\n",
            "Iteration 4800: [32.06747152]\n",
            "Iteration 4900: [32.06715162]\n",
            "Iteration 5000: [32.06686963]\n",
            "Iteration 5100: [32.06662106]\n",
            "Iteration 5200: [32.06640196]\n",
            "Iteration 5300: [32.06620882]\n",
            "Iteration 5400: [32.06603857]\n",
            "Iteration 5500: [32.06588851]\n",
            "Iteration 5600: [32.06575622]\n",
            "Iteration 5700: [32.06563962]\n",
            "Iteration 5800: [32.06553683]\n",
            "Iteration 5900: [32.06544623]\n",
            "Iteration 6000: [32.06536637]\n",
            "Iteration 6100: [32.06529597]\n",
            "Iteration 6200: [32.06523392]\n",
            "Iteration 6300: [32.06517922]\n",
            "Iteration 6400: [32.065131]\n",
            "Iteration 6500: [32.0650885]\n",
            "Iteration 6600: [32.06505103]\n",
            "Iteration 6700: [32.06501801]\n",
            "Iteration 6800: [32.0649889]\n",
            "Iteration 6900: [32.06496324]\n",
            "Iteration 7000: [32.06494062]\n",
            "Iteration 7100: [32.06492068]\n",
            "Iteration 7200: [32.06490311]\n",
            "Iteration 7300: [32.06488761]\n",
            "Iteration 7400: [32.06487396]\n",
            "Iteration 7500: [32.06486192]\n",
            "Iteration 7600: [32.06485131]\n",
            "Iteration 7700: [32.06484196]\n",
            "Iteration 7800: [32.06483371]\n",
            "Iteration 7900: [32.06482645]\n",
            "Iteration 8000: [32.06482004]\n",
            "Iteration 8100: [32.06481439]\n",
            "Iteration 8200: [32.06480942]\n",
            "Iteration 8300: [32.06480503]\n",
            "Iteration 8400: [32.06480116]\n",
            "Iteration 8500: [32.06479775]\n",
            "Iteration 8600: [32.06479475]\n",
            "Iteration 8700: [32.0647921]\n",
            "Iteration 8800: [32.06478976]\n",
            "Iteration 8900: [32.0647877]\n",
            "Iteration 9000: [32.06478589]\n",
            "Iteration 9100: [32.06478429]\n",
            "Iteration 9200: [32.06478288]\n",
            "Iteration 9300: [32.06478164]\n",
            "Iteration 9400: [32.06478054]\n",
            "Iteration 9500: [32.06477958]\n",
            "Iteration 9600: [32.06477873]\n",
            "Iteration 9700: [32.06477798]\n",
            "Iteration 9800: [32.06477731]\n",
            "Iteration 9900: [32.06477673]\n",
            "Train Error:  [32.06477622] , Test Error:  [58.41917441] alpha : 0.2\n",
            "Iteration 0: [136.96355552]\n",
            "Iteration 100: [33.76987819]\n",
            "Iteration 200: [33.04516442]\n",
            "Iteration 300: [32.79117946]\n",
            "Iteration 400: [32.63521425]\n",
            "Iteration 500: [32.52438193]\n",
            "Iteration 600: [32.43988437]\n",
            "Iteration 700: [32.37296613]\n",
            "Iteration 800: [32.31886496]\n",
            "Iteration 900: [32.27463966]\n",
            "Iteration 1000: [32.23827509]\n",
            "Iteration 1100: [32.20828185]\n",
            "Iteration 1200: [32.18350377]\n",
            "Iteration 1300: [32.16301686]\n",
            "Iteration 1400: [32.14607053]\n",
            "Iteration 1500: [32.13204971]\n",
            "Iteration 1600: [32.12044799]\n",
            "Iteration 1700: [32.11084738]\n",
            "Iteration 1800: [32.10290248]\n",
            "Iteration 1900: [32.09632764]\n",
            "Iteration 2000: [32.09088654]\n",
            "Iteration 2100: [32.08638367]\n",
            "Iteration 2200: [32.08265724]\n",
            "Iteration 2300: [32.07957336]\n",
            "Iteration 2400: [32.07702123]\n",
            "Iteration 2500: [32.07490917]\n",
            "Iteration 2600: [32.07316129]\n",
            "Iteration 2700: [32.0717148]\n",
            "Iteration 2800: [32.07051772]\n",
            "Iteration 2900: [32.06952706]\n",
            "Iteration 3000: [32.06870721]\n",
            "Iteration 3100: [32.06802873]\n",
            "Iteration 3200: [32.06746724]\n",
            "Iteration 3300: [32.06700257]\n",
            "Iteration 3400: [32.06661802]\n",
            "Iteration 3500: [32.06629978]\n",
            "Iteration 3600: [32.06603642]\n",
            "Iteration 3700: [32.06581846]\n",
            "Iteration 3800: [32.06563809]\n",
            "Iteration 3900: [32.06548882]\n",
            "Iteration 4000: [32.06536529]\n",
            "Iteration 4100: [32.06526305]\n",
            "Iteration 4200: [32.06517845]\n",
            "Iteration 4300: [32.06510843]\n",
            "Iteration 4400: [32.06505049]\n",
            "Iteration 4500: [32.06500254]\n",
            "Iteration 4600: [32.06496286]\n",
            "Iteration 4700: [32.06493002]\n",
            "Iteration 4800: [32.06490284]\n",
            "Iteration 4900: [32.06488035]\n",
            "Iteration 5000: [32.06486173]\n",
            "Iteration 5100: [32.06484633]\n",
            "Iteration 5200: [32.06483358]\n",
            "Iteration 5300: [32.06482303]\n",
            "Iteration 5400: [32.0648143]\n",
            "Iteration 5500: [32.06480707]\n",
            "Iteration 5600: [32.06480109]\n",
            "Iteration 5700: [32.06479615]\n",
            "Iteration 5800: [32.06479205]\n",
            "Iteration 5900: [32.06478866]\n",
            "Iteration 6000: [32.06478586]\n",
            "Iteration 6100: [32.06478354]\n",
            "Iteration 6200: [32.06478162]\n",
            "Iteration 6300: [32.06478003]\n",
            "Iteration 6400: [32.06477871]\n",
            "Iteration 6500: [32.06477762]\n",
            "Iteration 6600: [32.06477672]\n",
            "Iteration 6700: [32.06477598]\n",
            "Iteration 6800: [32.06477536]\n",
            "Iteration 6900: [32.06477485]\n",
            "Iteration 7000: [32.06477442]\n",
            "Iteration 7100: [32.06477408]\n",
            "Iteration 7200: [32.06477379]\n",
            "Iteration 7300: [32.06477355]\n",
            "Iteration 7400: [32.06477335]\n",
            "Iteration 7500: [32.06477318]\n",
            "Iteration 7600: [32.06477305]\n",
            "Iteration 7700: [32.06477294]\n",
            "Iteration 7800: [32.06477284]\n",
            "Iteration 7900: [32.06477277]\n",
            "Iteration 8000: [32.0647727]\n",
            "Iteration 8100: [32.06477265]\n",
            "Iteration 8200: [32.06477261]\n",
            "Iteration 8300: [32.06477257]\n",
            "Iteration 8400: [32.06477254]\n",
            "Iteration 8500: [32.06477252]\n",
            "Iteration 8600: [32.06477249]\n",
            "Iteration 8700: [32.06477248]\n",
            "Iteration 8800: [32.06477246]\n",
            "Iteration 8900: [32.06477245]\n",
            "Iteration 9000: [32.06477244]\n",
            "Iteration 9100: [32.06477243]\n",
            "Iteration 9200: [32.06477243]\n",
            "Iteration 9300: [32.06477242]\n",
            "Iteration 9400: [32.06477242]\n",
            "Iteration 9500: [32.06477241]\n",
            "Iteration 9600: [32.06477241]\n",
            "Iteration 9700: [32.06477241]\n",
            "Iteration 9800: [32.06477241]\n",
            "Iteration 9900: [32.0647724]\n",
            "Train Error:  [32.0647724] , Test Error:  [58.41690439] alpha : 0.30000000000000004\n",
            "Iteration 0: [65.87388786]\n",
            "Iteration 100: [33.38249158]\n",
            "Iteration 200: [32.8590414]\n",
            "Iteration 300: [32.63464119]\n",
            "Iteration 400: [32.49345257]\n",
            "Iteration 500: [32.39337123]\n",
            "Iteration 600: [32.31863701]\n",
            "Iteration 700: [32.26154504]\n",
            "Iteration 800: [32.21750251]\n",
            "Iteration 900: [32.18338601]\n",
            "Iteration 1000: [32.15691254]\n",
            "Iteration 1100: [32.13635489]\n",
            "Iteration 1200: [32.12038626]\n",
            "Iteration 1300: [32.10798067]\n",
            "Iteration 1400: [32.09834259]\n",
            "Iteration 1500: [32.09085447]\n",
            "Iteration 1600: [32.08503667]\n",
            "Iteration 1700: [32.08051658]\n",
            "Iteration 1800: [32.07700473]\n",
            "Iteration 1900: [32.07427623]\n",
            "Iteration 2000: [32.07215633]\n",
            "Iteration 2100: [32.0705093]\n",
            "Iteration 2200: [32.06922964]\n",
            "Iteration 2300: [32.06823542]\n",
            "Iteration 2400: [32.06746297]\n",
            "Iteration 2500: [32.06686282]\n",
            "Iteration 2600: [32.06639654]\n",
            "Iteration 2700: [32.06603426]\n",
            "Iteration 2800: [32.06575279]\n",
            "Iteration 2900: [32.06553411]\n",
            "Iteration 3000: [32.0653642]\n",
            "Iteration 3100: [32.0652322]\n",
            "Iteration 3200: [32.06512964]\n",
            "Iteration 3300: [32.06504995]\n",
            "Iteration 3400: [32.06498804]\n",
            "Iteration 3500: [32.06493994]\n",
            "Iteration 3600: [32.06490257]\n",
            "Iteration 3700: [32.06487353]\n",
            "Iteration 3800: [32.06485097]\n",
            "Iteration 3900: [32.06483345]\n",
            "Iteration 4000: [32.06481983]\n",
            "Iteration 4100: [32.06480925]\n",
            "Iteration 4200: [32.06480103]\n",
            "Iteration 4300: [32.06479464]\n",
            "Iteration 4400: [32.06478968]\n",
            "Iteration 4500: [32.06478582]\n",
            "Iteration 4600: [32.06478283]\n",
            "Iteration 4700: [32.0647805]\n",
            "Iteration 4800: [32.06477869]\n",
            "Iteration 4900: [32.06477729]\n",
            "Iteration 5000: [32.0647762]\n",
            "Iteration 5100: [32.06477535]\n",
            "Iteration 5200: [32.06477469]\n",
            "Iteration 5300: [32.06477418]\n",
            "Iteration 5400: [32.06477378]\n",
            "Iteration 5500: [32.06477347]\n",
            "Iteration 5600: [32.06477323]\n",
            "Iteration 5700: [32.06477305]\n",
            "Iteration 5800: [32.0647729]\n",
            "Iteration 5900: [32.06477279]\n",
            "Iteration 6000: [32.0647727]\n",
            "Iteration 6100: [32.06477263]\n",
            "Iteration 6200: [32.06477258]\n",
            "Iteration 6300: [32.06477254]\n",
            "Iteration 6400: [32.06477251]\n",
            "Iteration 6500: [32.06477248]\n",
            "Iteration 6600: [32.06477246]\n",
            "Iteration 6700: [32.06477245]\n",
            "Iteration 6800: [32.06477244]\n",
            "Iteration 6900: [32.06477243]\n",
            "Iteration 7000: [32.06477242]\n",
            "Iteration 7100: [32.06477242]\n",
            "Iteration 7200: [32.06477241]\n",
            "Iteration 7300: [32.06477241]\n",
            "Iteration 7400: [32.06477241]\n",
            "Iteration 7500: [32.0647724]\n",
            "Iteration 7600: [32.0647724]\n",
            "Iteration 7700: [32.0647724]\n",
            "Iteration 7800: [32.0647724]\n",
            "Iteration 7900: [32.0647724]\n",
            "Iteration 8000: [32.0647724]\n",
            "Iteration 8100: [32.0647724]\n",
            "Iteration 8200: [32.0647724]\n",
            "Iteration 8300: [32.0647724]\n",
            "Iteration 8400: [32.0647724]\n",
            "Iteration 8500: [32.0647724]\n",
            "Iteration 8600: [32.0647724]\n",
            "Iteration 8700: [32.0647724]\n",
            "Iteration 8800: [32.0647724]\n",
            "Iteration 8900: [32.0647724]\n",
            "Iteration 9000: [32.0647724]\n",
            "Iteration 9100: [32.0647724]\n",
            "Iteration 9200: [32.0647724]\n",
            "Iteration 9300: [32.0647724]\n",
            "Iteration 9400: [32.0647724]\n",
            "Iteration 9500: [32.0647724]\n",
            "Iteration 9600: [32.0647724]\n",
            "Iteration 9700: [32.0647724]\n",
            "Iteration 9800: [32.0647724]\n",
            "Iteration 9900: [32.0647724]\n",
            "Train Error:  [32.0647724] , Test Error:  [58.41680783] alpha : 0.4\n",
            "Iteration 0: [72.84833334]\n",
            "Iteration 100: [33.17683262]\n",
            "Iteration 200: [32.73071367]\n",
            "Iteration 300: [32.52353099]\n",
            "Iteration 400: [32.39307943]\n",
            "Iteration 500: [32.30270929]\n",
            "Iteration 600: [32.23794966]\n",
            "Iteration 700: [32.19099629]\n",
            "Iteration 800: [32.15681759]\n",
            "Iteration 900: [32.13190457]\n",
            "Iteration 1000: [32.11373717]\n",
            "Iteration 1100: [32.10048691]\n",
            "Iteration 1200: [32.09082244]\n",
            "Iteration 1300: [32.08377324]\n",
            "Iteration 1400: [32.07863158]\n",
            "Iteration 1500: [32.07488127]\n",
            "Iteration 1600: [32.07214579]\n",
            "Iteration 1700: [32.07015054]\n",
            "Iteration 1800: [32.06869521]\n",
            "Iteration 1900: [32.06763369]\n",
            "Iteration 2000: [32.06685942]\n",
            "Iteration 2100: [32.06629467]\n",
            "Iteration 2200: [32.06588274]\n",
            "Iteration 2300: [32.06558228]\n",
            "Iteration 2400: [32.06536312]\n",
            "Iteration 2500: [32.06520327]\n",
            "Iteration 2600: [32.06508668]\n",
            "Iteration 2700: [32.06500163]\n",
            "Iteration 2800: [32.0649396]\n",
            "Iteration 2900: [32.06489436]\n",
            "Iteration 3000: [32.06486135]\n",
            "Iteration 3100: [32.06483728]\n",
            "Iteration 3200: [32.06481972]\n",
            "Iteration 3300: [32.06480692]\n",
            "Iteration 3400: [32.06479758]\n",
            "Iteration 3500: [32.06479076]\n",
            "Iteration 3600: [32.06478579]\n",
            "Iteration 3700: [32.06478217]\n",
            "Iteration 3800: [32.06477952]\n",
            "Iteration 3900: [32.06477759]\n",
            "Iteration 4000: [32.06477619]\n",
            "Iteration 4100: [32.06477516]\n",
            "Iteration 4200: [32.06477441]\n",
            "Iteration 4300: [32.06477387]\n",
            "Iteration 4400: [32.06477347]\n",
            "Iteration 4500: [32.06477318]\n",
            "Iteration 4600: [32.06477297]\n",
            "Iteration 4700: [32.06477281]\n",
            "Iteration 4800: [32.0647727]\n",
            "Iteration 4900: [32.06477262]\n",
            "Iteration 5000: [32.06477256]\n",
            "Iteration 5100: [32.06477251]\n",
            "Iteration 5200: [32.06477248]\n",
            "Iteration 5300: [32.06477246]\n",
            "Iteration 5400: [32.06477244]\n",
            "Iteration 5500: [32.06477243]\n",
            "Iteration 5600: [32.06477242]\n",
            "Iteration 5700: [32.06477241]\n",
            "Iteration 5800: [32.06477241]\n",
            "Iteration 5900: [32.06477241]\n",
            "Iteration 6000: [32.0647724]\n",
            "Iteration 6100: [32.0647724]\n",
            "Iteration 6200: [32.0647724]\n",
            "Iteration 6300: [32.0647724]\n",
            "Iteration 6400: [32.0647724]\n",
            "Iteration 6500: [32.0647724]\n",
            "Iteration 6600: [32.0647724]\n",
            "Iteration 6700: [32.0647724]\n",
            "Iteration 6800: [32.0647724]\n",
            "Iteration 6900: [32.0647724]\n",
            "Iteration 7000: [32.0647724]\n",
            "Iteration 7100: [32.0647724]\n",
            "Iteration 7200: [32.0647724]\n",
            "Iteration 7300: [32.0647724]\n",
            "Iteration 7400: [32.0647724]\n",
            "Iteration 7500: [32.0647724]\n",
            "Iteration 7600: [32.0647724]\n",
            "Iteration 7700: [32.0647724]\n",
            "Iteration 7800: [32.0647724]\n",
            "Iteration 7900: [32.0647724]\n",
            "Iteration 8000: [32.0647724]\n",
            "Iteration 8100: [32.0647724]\n",
            "Iteration 8200: [32.0647724]\n",
            "Iteration 8300: [32.0647724]\n",
            "Iteration 8400: [32.0647724]\n",
            "Iteration 8500: [32.0647724]\n",
            "Iteration 8600: [32.0647724]\n",
            "Iteration 8700: [32.0647724]\n",
            "Iteration 8800: [32.0647724]\n",
            "Iteration 8900: [32.0647724]\n",
            "Iteration 9000: [32.0647724]\n",
            "Iteration 9100: [32.0647724]\n",
            "Iteration 9200: [32.0647724]\n",
            "Iteration 9300: [32.0647724]\n",
            "Iteration 9400: [32.0647724]\n",
            "Iteration 9500: [32.0647724]\n",
            "Iteration 9600: [32.0647724]\n",
            "Iteration 9700: [32.0647724]\n",
            "Iteration 9800: [32.0647724]\n",
            "Iteration 9900: [32.0647724]\n",
            "Train Error:  [32.0647724] , Test Error:  [58.41680372] alpha : 0.5\n",
            "Iteration 0: [157.88689197]\n",
            "Iteration 100: [33.04024768]\n",
            "Iteration 200: [32.63349773]\n",
            "Iteration 300: [32.43887913]\n",
            "Iteration 400: [32.3181816]\n",
            "Iteration 500: [32.23778711]\n",
            "Iteration 600: [32.18315072]\n",
            "Iteration 700: [32.14581471]\n",
            "Iteration 800: [32.12026294]\n",
            "Iteration 900: [32.10276895]\n",
            "Iteration 1000: [32.09079043]\n",
            "Iteration 1100: [32.08258822]\n",
            "Iteration 1200: [32.07697178]\n",
            "Iteration 1300: [32.07312592]\n",
            "Iteration 1400: [32.07049247]\n",
            "Iteration 1500: [32.06868922]\n",
            "Iteration 1600: [32.06745444]\n",
            "Iteration 1700: [32.06660893]\n",
            "Iteration 1800: [32.06602996]\n",
            "Iteration 1900: [32.06563351]\n",
            "Iteration 2000: [32.06536205]\n",
            "Iteration 2100: [32.06517616]\n",
            "Iteration 2200: [32.06504887]\n",
            "Iteration 2300: [32.06496171]\n",
            "Iteration 2400: [32.06490203]\n",
            "Iteration 2500: [32.06486116]\n",
            "Iteration 2600: [32.06483318]\n",
            "Iteration 2700: [32.06481402]\n",
            "Iteration 2800: [32.0648009]\n",
            "Iteration 2900: [32.06479191]\n",
            "Iteration 3000: [32.06478576]\n",
            "Iteration 3100: [32.06478155]\n",
            "Iteration 3200: [32.06477866]\n",
            "Iteration 3300: [32.06477669]\n",
            "Iteration 3400: [32.06477533]\n",
            "Iteration 3500: [32.06477441]\n",
            "Iteration 3600: [32.06477377]\n",
            "Iteration 3700: [32.06477334]\n",
            "Iteration 3800: [32.06477304]\n",
            "Iteration 3900: [32.06477284]\n",
            "Iteration 4000: [32.0647727]\n",
            "Iteration 4100: [32.0647726]\n",
            "Iteration 4200: [32.06477254]\n",
            "Iteration 4300: [32.06477249]\n",
            "Iteration 4400: [32.06477246]\n",
            "Iteration 4500: [32.06477244]\n",
            "Iteration 4600: [32.06477243]\n",
            "Iteration 4700: [32.06477242]\n",
            "Iteration 4800: [32.06477241]\n",
            "Iteration 4900: [32.06477241]\n",
            "Iteration 5000: [32.0647724]\n",
            "Iteration 5100: [32.0647724]\n",
            "Iteration 5200: [32.0647724]\n",
            "Iteration 5300: [32.0647724]\n",
            "Iteration 5400: [32.0647724]\n",
            "Iteration 5500: [32.0647724]\n",
            "Iteration 5600: [32.0647724]\n",
            "Iteration 5700: [32.0647724]\n",
            "Iteration 5800: [32.0647724]\n",
            "Iteration 5900: [32.0647724]\n",
            "Iteration 6000: [32.0647724]\n",
            "Iteration 6100: [32.0647724]\n",
            "Iteration 6200: [32.0647724]\n",
            "Iteration 6300: [32.0647724]\n",
            "Iteration 6400: [32.0647724]\n",
            "Iteration 6500: [32.0647724]\n",
            "Iteration 6600: [32.0647724]\n",
            "Iteration 6700: [32.0647724]\n",
            "Iteration 6800: [32.0647724]\n",
            "Iteration 6900: [32.0647724]\n",
            "Iteration 7000: [32.0647724]\n",
            "Iteration 7100: [32.0647724]\n",
            "Iteration 7200: [32.0647724]\n",
            "Iteration 7300: [32.0647724]\n",
            "Iteration 7400: [32.0647724]\n",
            "Iteration 7500: [32.0647724]\n",
            "Iteration 7600: [32.0647724]\n",
            "Iteration 7700: [32.0647724]\n",
            "Iteration 7800: [32.0647724]\n",
            "Iteration 7900: [32.0647724]\n",
            "Iteration 8000: [32.0647724]\n",
            "Iteration 8100: [32.0647724]\n",
            "Iteration 8200: [32.0647724]\n",
            "Iteration 8300: [32.0647724]\n",
            "Iteration 8400: [32.0647724]\n",
            "Iteration 8500: [32.0647724]\n",
            "Iteration 8600: [32.0647724]\n",
            "Iteration 8700: [32.0647724]\n",
            "Iteration 8800: [32.0647724]\n",
            "Iteration 8900: [32.0647724]\n",
            "Iteration 9000: [32.0647724]\n",
            "Iteration 9100: [32.0647724]\n",
            "Iteration 9200: [32.0647724]\n",
            "Iteration 9300: [32.0647724]\n",
            "Iteration 9400: [32.0647724]\n",
            "Iteration 9500: [32.0647724]\n",
            "Iteration 9600: [32.0647724]\n",
            "Iteration 9700: [32.0647724]\n",
            "Iteration 9800: [32.0647724]\n",
            "Iteration 9900: [32.0647724]\n",
            "Train Error:  [32.0647724] , Test Error:  [58.41680355] alpha : 0.6\n",
            "Iteration 0: [320.98956374]\n",
            "Iteration 100: [32.93753988]\n",
            "Iteration 200: [32.55591338]\n",
            "Iteration 300: [32.37187271]\n",
            "Iteration 400: [32.2610008]\n",
            "Iteration 500: [32.19074819]\n",
            "Iteration 600: [32.14572955]\n",
            "Iteration 700: [32.11681023]\n",
            "Iteration 800: [32.09822299]\n",
            "Iteration 900: [32.0862751]\n",
            "Iteration 1000: [32.07859478]\n",
            "Iteration 1100: [32.07365772]\n",
            "Iteration 1200: [32.07048408]\n",
            "Iteration 1300: [32.06844399]\n",
            "Iteration 1400: [32.06713257]\n",
            "Iteration 1500: [32.06628957]\n",
            "Iteration 1600: [32.06574767]\n",
            "Iteration 1700: [32.06539932]\n",
            "Iteration 1800: [32.0651754]\n",
            "Iteration 1900: [32.06503145]\n",
            "Iteration 2000: [32.06493892]\n",
            "Iteration 2100: [32.06487944]\n",
            "Iteration 2200: [32.06484121]\n",
            "Iteration 2300: [32.06481663]\n",
            "Iteration 2400: [32.06480083]\n",
            "Iteration 2500: [32.06479067]\n",
            "Iteration 2600: [32.06478415]\n",
            "Iteration 2700: [32.06477995]\n",
            "Iteration 2800: [32.06477725]\n",
            "Iteration 2900: [32.06477552]\n",
            "Iteration 3000: [32.0647744]\n",
            "Iteration 3100: [32.06477369]\n",
            "Iteration 3200: [32.06477323]\n",
            "Iteration 3300: [32.06477293]\n",
            "Iteration 3400: [32.06477274]\n",
            "Iteration 3500: [32.06477262]\n",
            "Iteration 3600: [32.06477254]\n",
            "Iteration 3700: [32.06477249]\n",
            "Iteration 3800: [32.06477246]\n",
            "Iteration 3900: [32.06477243]\n",
            "Iteration 4000: [32.06477242]\n",
            "Iteration 4100: [32.06477241]\n",
            "Iteration 4200: [32.06477241]\n",
            "Iteration 4300: [32.0647724]\n",
            "Iteration 4400: [32.0647724]\n",
            "Iteration 4500: [32.0647724]\n",
            "Iteration 4600: [32.0647724]\n",
            "Iteration 4700: [32.0647724]\n",
            "Iteration 4800: [32.0647724]\n",
            "Iteration 4900: [32.0647724]\n",
            "Iteration 5000: [32.0647724]\n",
            "Iteration 5100: [32.0647724]\n",
            "Iteration 5200: [32.0647724]\n",
            "Iteration 5300: [32.0647724]\n",
            "Iteration 5400: [32.0647724]\n",
            "Iteration 5500: [32.0647724]\n",
            "Iteration 5600: [32.0647724]\n",
            "Iteration 5700: [32.0647724]\n",
            "Iteration 5800: [32.0647724]\n",
            "Iteration 5900: [32.0647724]\n",
            "Iteration 6000: [32.0647724]\n",
            "Iteration 6100: [32.0647724]\n",
            "Iteration 6200: [32.0647724]\n",
            "Iteration 6300: [32.0647724]\n",
            "Iteration 6400: [32.0647724]\n",
            "Iteration 6500: [32.0647724]\n",
            "Iteration 6600: [32.0647724]\n",
            "Iteration 6700: [32.0647724]\n",
            "Iteration 6800: [32.0647724]\n",
            "Iteration 6900: [32.0647724]\n",
            "Iteration 7000: [32.0647724]\n",
            "Iteration 7100: [32.0647724]\n",
            "Iteration 7200: [32.0647724]\n",
            "Iteration 7300: [32.0647724]\n",
            "Iteration 7400: [32.0647724]\n",
            "Iteration 7500: [32.0647724]\n",
            "Iteration 7600: [32.0647724]\n",
            "Iteration 7700: [32.0647724]\n",
            "Iteration 7800: [32.0647724]\n",
            "Iteration 7900: [32.0647724]\n",
            "Iteration 8000: [32.0647724]\n",
            "Iteration 8100: [32.0647724]\n",
            "Iteration 8200: [32.0647724]\n",
            "Iteration 8300: [32.0647724]\n",
            "Iteration 8400: [32.0647724]\n",
            "Iteration 8500: [32.0647724]\n",
            "Iteration 8600: [32.0647724]\n",
            "Iteration 8700: [32.0647724]\n",
            "Iteration 8800: [32.0647724]\n",
            "Iteration 8900: [32.0647724]\n",
            "Iteration 9000: [32.0647724]\n",
            "Iteration 9100: [32.0647724]\n",
            "Iteration 9200: [32.0647724]\n",
            "Iteration 9300: [32.0647724]\n",
            "Iteration 9400: [32.0647724]\n",
            "Iteration 9500: [32.0647724]\n",
            "Iteration 9600: [32.0647724]\n",
            "Iteration 9700: [32.0647724]\n",
            "Iteration 9800: [32.0647724]\n",
            "Iteration 9900: [32.0647724]\n",
            "Train Error:  [32.0647724] , Test Error:  [58.41680354] alpha : 0.7\n",
            "Iteration 0: [562.15634866]\n",
            "Iteration 100: [32.85500396]\n",
            "Iteration 200: [32.49189208]\n",
            "Iteration 300: [32.31772684]\n",
            "Iteration 400: [32.21691933]\n",
            "Iteration 500: [32.15653312]\n",
            "Iteration 600: [32.1201398]\n",
            "Iteration 700: [32.09818319]\n",
            "Iteration 800: [32.08493402]\n",
            "Iteration 900: [32.07693889]\n",
            "Iteration 1000: [32.07211425]\n",
            "Iteration 1100: [32.06920282]\n",
            "Iteration 1200: [32.06744593]\n",
            "Iteration 1300: [32.06638574]\n",
            "Iteration 1400: [32.06574596]\n",
            "Iteration 1500: [32.06535989]\n",
            "Iteration 1600: [32.06512692]\n",
            "Iteration 1700: [32.06498633]\n",
            "Iteration 1800: [32.0649015]\n",
            "Iteration 1900: [32.0648503]\n",
            "Iteration 2000: [32.06481941]\n",
            "Iteration 2100: [32.06480077]\n",
            "Iteration 2200: [32.06478952]\n",
            "Iteration 2300: [32.06478273]\n",
            "Iteration 2400: [32.06477863]\n",
            "Iteration 2500: [32.06477616]\n",
            "Iteration 2600: [32.06477467]\n",
            "Iteration 2700: [32.06477377]\n",
            "Iteration 2800: [32.06477322]\n",
            "Iteration 2900: [32.0647729]\n",
            "Iteration 3000: [32.0647727]\n",
            "Iteration 3100: [32.06477258]\n",
            "Iteration 3200: [32.06477251]\n",
            "Iteration 3300: [32.06477246]\n",
            "Iteration 3400: [32.06477244]\n",
            "Iteration 3500: [32.06477242]\n",
            "Iteration 3600: [32.06477241]\n",
            "Iteration 3700: [32.06477241]\n",
            "Iteration 3800: [32.0647724]\n",
            "Iteration 3900: [32.0647724]\n",
            "Iteration 4000: [32.0647724]\n",
            "Iteration 4100: [32.0647724]\n",
            "Iteration 4200: [32.0647724]\n",
            "Iteration 4300: [32.0647724]\n",
            "Iteration 4400: [32.0647724]\n",
            "Iteration 4500: [32.0647724]\n",
            "Iteration 4600: [32.0647724]\n",
            "Iteration 4700: [32.0647724]\n",
            "Iteration 4800: [32.0647724]\n",
            "Iteration 4900: [32.0647724]\n",
            "Iteration 5000: [32.0647724]\n",
            "Iteration 5100: [32.0647724]\n",
            "Iteration 5200: [32.0647724]\n",
            "Iteration 5300: [32.0647724]\n",
            "Iteration 5400: [32.0647724]\n",
            "Iteration 5500: [32.0647724]\n",
            "Iteration 5600: [32.0647724]\n",
            "Iteration 5700: [32.0647724]\n",
            "Iteration 5800: [32.0647724]\n",
            "Iteration 5900: [32.0647724]\n",
            "Iteration 6000: [32.0647724]\n",
            "Iteration 6100: [32.0647724]\n",
            "Iteration 6200: [32.0647724]\n",
            "Iteration 6300: [32.0647724]\n",
            "Iteration 6400: [32.0647724]\n",
            "Iteration 6500: [32.0647724]\n",
            "Iteration 6600: [32.0647724]\n",
            "Iteration 6700: [32.0647724]\n",
            "Iteration 6800: [32.0647724]\n",
            "Iteration 6900: [32.0647724]\n",
            "Iteration 7000: [32.0647724]\n",
            "Iteration 7100: [32.0647724]\n",
            "Iteration 7200: [32.0647724]\n",
            "Iteration 7300: [32.0647724]\n",
            "Iteration 7400: [32.0647724]\n",
            "Iteration 7500: [32.0647724]\n",
            "Iteration 7600: [32.0647724]\n",
            "Iteration 7700: [32.0647724]\n",
            "Iteration 7800: [32.0647724]\n",
            "Iteration 7900: [32.0647724]\n",
            "Iteration 8000: [32.0647724]\n",
            "Iteration 8100: [32.0647724]\n",
            "Iteration 8200: [32.0647724]\n",
            "Iteration 8300: [32.0647724]\n",
            "Iteration 8400: [32.0647724]\n",
            "Iteration 8500: [32.0647724]\n",
            "Iteration 8600: [32.0647724]\n",
            "Iteration 8700: [32.0647724]\n",
            "Iteration 8800: [32.0647724]\n",
            "Iteration 8900: [32.0647724]\n",
            "Iteration 9000: [32.0647724]\n",
            "Iteration 9100: [32.0647724]\n",
            "Iteration 9200: [32.0647724]\n",
            "Iteration 9300: [32.0647724]\n",
            "Iteration 9400: [32.0647724]\n",
            "Iteration 9500: [32.0647724]\n",
            "Iteration 9600: [32.0647724]\n",
            "Iteration 9700: [32.0647724]\n",
            "Iteration 9800: [32.0647724]\n",
            "Iteration 9900: [32.0647724]\n",
            "Train Error:  [32.0647724] , Test Error:  [58.41680354] alpha : 0.7999999999999999\n",
            "Train Error Before:  [65.16475777] , Test Error Before:  [101.48269921] Alpha Before :  0.00019097790427776226\n",
            "Minimum Train Error:  [32.0647724] , Minimum Test Error:  [58.41680354] Minimum Alpha:  0.7999999999999999\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "\"\"\"\n",
        "Manually choose the hyperparameters (learning rate and number of features) and train the model.\n",
        "Then compare the performance with random chosen hyperparameters and manually chosen hyperparameters.\n",
        "\"\"\"\n",
        "\n",
        "### START CODE HERE ###\n",
        "filepath=\"Taiwan_House.csv\"\n",
        "new_X, new_y = load_data(filepath)\n",
        "new_X = min_max_scaler(new_X)\n",
        "new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(new_X, new_y, test_size=0.25, random_state=42)\n",
        "new_initial_w = np.random.normal(0,1,size=(1,new_X_train.shape[1]))\n",
        "new_initial_b = np.random.normal(0,1)\n",
        "num_iters = 10000\n",
        "new_alpha=0.1\n",
        "min_train_error = 1e9\n",
        "min_test_error = 1e9\n",
        "min_alpha = 2\n",
        "while new_alpha<=0.8:\n",
        "  wnew,bnew,loss_hist_new = batch_gradient_descent(new_X_train ,new_y_train, new_initial_w, new_initial_b, new_alpha, num_iters)\n",
        "  new_train_error = loss_function(new_X_train,new_y_train,wnew,bnew)\n",
        "  new_test_error = loss_function(new_X_test,new_y_test,wnew,bnew)\n",
        "\n",
        "  if new_test_error<min_test_error:\n",
        "    min_train_error = new_train_error\n",
        "    min_test_error = new_test_error\n",
        "    min_alpha = new_alpha\n",
        "\n",
        "  print(\"Train Error: \",new_train_error, \", Test Error: \",new_test_error,\"alpha :\",new_alpha)\n",
        "  new_alpha=new_alpha+0.1\n",
        "\n",
        "print(\"Train Error Before: \",train_error, \", Test Error Before: \",test_error,\"Alpha Before : \",alpha)\n",
        "print(\"Minimum Train Error: \",min_train_error, \", Minimum Test Error: \",min_test_error,\"Minimum Alpha: \",min_alpha)\n",
        "\n",
        "### END CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ0CviUnZui0"
      },
      "source": [
        "## Part 2: Logistic Regression\n",
        "\n",
        "### Problem Statement\n",
        "A Food grain industry requires an efficient classification system to help in sorting food grain species. You have to develop a Logistic Regression model for this purpose.\n",
        " Given various features of a rice grain such as area, perimeter, axis lengths etc. as input features, the task is to build a logistic regression model to predict the species of the food grain.\n",
        "\n",
        "### Dataset Description\n",
        "\n",
        "**For Even Roll Number Students**\n",
        "\n",
        "Dataset Filename: Rice_Classification.csv\n",
        "\n",
        "Attribute Information:\n",
        "+ Area: Returns the number of pixels within the boundaries of the rice grain.\n",
        "+ Perimeter: Calculates the circumference by calculating the distance between pixels around the boundaries of the rice grain.\n",
        "+ Major Axis Length: The longest line that can be drawn on the rice grain, i.e. the main axis distance, gives.\n",
        "+ Minor Axis Length: The shortest line that can be drawn on the rice grain, i.e. the small axis distance, gives.\n",
        "+ Eccentricity: It measures how round the ellipse, which has the same moments as the rice grain, is.\n",
        "+ Convex Area: Returns the pixel count of the smallest convex shell of the region formed by the rice grain.\n",
        "+ Extent: Returns the ratio of the region formed by the rice grain to the bounding box pixels.\n",
        "\n",
        "Target Variable: Class: Cammeo and Osmancik\n",
        "\n",
        "**For Odd Roll Number Students**\n",
        "\n",
        "Dataset Filename: Pumpkin_Seeds_Dataset.csv\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "+ Area: Represents the number of pixels within the boundaries of the pumpkin seed.\n",
        "\n",
        "+ Perimeter: Calculates the circumference by measuring the distance between pixels around the boundary of the pumpkin seed.\n",
        "\n",
        "+ Major Axis Length: The longest line that can be drawn on the pumpkin seed, representing the main axis distance.\n",
        "\n",
        "+ Minor Axis Length: The shortest line that can be drawn on the pumpkin seed, representing the minor axis distance.\n",
        "\n",
        "+ Convex Area: Returns the pixel count of the smallest convex shell that can contain the pumpkin seed.\n",
        "\n",
        "+ Equivalent Diameter: Diameter of a circle with the same area as the pumpkin seed.\n",
        "\n",
        "+ Eccentricity: This measures how round the ellipse, which has the same moments as the pumpkin seed has.\n",
        "\n",
        "+ Solidity: This is the ratio of the area of the pumpkin seed to the area of its convex hull. It measures the extent to which the shape is convex.\n",
        "\n",
        "+ Extent: Returns the ratio of the area of the pumpkin seed to the area of its bounding box.\n",
        "\n",
        "+ Roundness: Measure of how closely the shape of the pumpkin seed approaches that of a circle.\n",
        "\n",
        "+ Aspect Ratio: Ratio of the major axis length to the minor axis length.\n",
        "\n",
        "+ Compactness: Measure of the shape's compactness, which is the shape's deviation from being a perfect circle. In essence, compactness quantifies how efficiently an object's area is packed within its perimeter.\n",
        "\n",
        "Target Variable: Class: Çerçevelik and Ürgüp Sivrisi\n",
        "\n",
        "\n",
        "\n",
        "These are the following steps or functions that you have to complete to create and train the linear regression model:\n",
        "1. Reading the data\n",
        "2. Creating the sigmoid function\n",
        "2. Computing the loss function\n",
        "3. Computing the gradient of the loss\n",
        "4. Training the model using Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 616,
      "metadata": {
        "id": "T27XOBZrRZ4M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import logsumexp\n",
        "import copy\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3luUk4L0WO_"
      },
      "source": [
        "### 2.1. Reading the data\n",
        "\n",
        "In the function ```load_data```, you have to read data from the file, store it in a dataframe and split the data from the dataframe into two numpy arrays X and y.\n",
        "\n",
        "**X** : data of the input features\n",
        "\n",
        "**y**  : data of the class labels\n",
        "\n",
        "The class labels in **y** should be replaced with '0' and '1', for corresponding classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 587,
      "metadata": {
        "id": "7-RV0JgEZi1h"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X:  (3810, 7) Shape of y:  (3810,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def load_data(filepath):\n",
        "    \"\"\"\n",
        "    This function loads the data into a pandas dataframe and converts it into X and y numpy arrays\n",
        "    y should be a binary numpy array with values 0 and 1, for 2 different classes\n",
        "    Args:\n",
        "        filepath: File path as a string\n",
        "    Returns:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "        y: Target variable of the shape (# of sample,) with values 0 and 1, for 2 different classes\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    df = pd.read_csv(filepath)\n",
        "    x_attr = df[['Area', 'Perimeter', 'Major_Axis_Length', 'Minor_Axis_Length', 'Eccentricity', 'Convex_Area', 'Extent']]\n",
        "    X = x_attr.to_numpy()\n",
        "    y_attr = df['Class']\n",
        "    y = y_attr.map({'Osmancik': 1, 'Cammeo': 0}).to_numpy()\n",
        "    ### END CODE HERE ###\n",
        "   \n",
        "    return X,y\n",
        "\n",
        "filepath = None\n",
        "### START CODE HERE ###\n",
        "## set the file path\n",
        "filepath=\"Rice_Classification.csv\"\n",
        "### END CODE HERE ###\n",
        "\n",
        "X, y = load_data(filepath)\n",
        "\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLGPG1YU-C47"
      },
      "source": [
        "We will not use all the features from X.\n",
        "\n",
        "For Even Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r between 4 and 7 (both inclusive) randomly. Use the first r features of the numpy array X.\n",
        "\n",
        "For Odd Roll Number Students: Set the last two digits of your roll number as the random seed and pick a number r between 8 and 12 (both inclusive) randomly. Use the first r features of the numpy array X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 588,
      "metadata": {
        "id": "JNsDb2Ae94s6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X:  (3810, 5) Shape of y:  (3810,)\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def random_feature_selection(X):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of samples, # of input features)\n",
        "    Returns:\n",
        "        X_new: New input data of the shape (# of samples, r) containg only the first r features from X\n",
        "    \"\"\"\n",
        "\n",
        "    X_new = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    np.random.seed(32)\n",
        "    r = np.random.randint(4,7)\n",
        "    X_new = X[:,:r]\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return X_new\n",
        "\n",
        "X = random_feature_selection(X)\n",
        "print(\"Shape of X: \",X.shape, \"Shape of y: \",y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4u-qBe34wC9"
      },
      "source": [
        "We need to pre-process the data. We are using min-max scaler to scale the input data ($X$).\n",
        "\n",
        "After that, we split the data (```X``` and ```y```) into a training dataset (```X_train``` and ```y_train```) and test dataset (```X_test``` and ```y_test```)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 589,
      "metadata": {
        "id": "YTO4etePa0i1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train:  (2857, 5) Shape of y_train:  (2857,)\n",
            "Shape of X_test:  (953, 5) Shape of y_test:  (953,)\n"
          ]
        }
      ],
      "source": [
        "## Data scaling and train-test split\n",
        "\n",
        "def train_test_split(X, y, test_size=0.25, random_state=None):\n",
        "    if random_state is not None:\n",
        "        np.random.seed(random_state)\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    split_index = int(X.shape[0] * (1 - test_size))\n",
        "\n",
        "    train_indices = indices[:split_index]\n",
        "    test_indices = indices[split_index:]\n",
        "\n",
        "    X_train = X[train_indices]\n",
        "    X_test = X[test_indices]\n",
        "    y_train = y[train_indices]\n",
        "    y_test = y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def min_max_scaler(X, feature_range=(0, 1)):\n",
        "    X_min = np.min(X, axis=0)\n",
        "    X_max = np.max(X, axis=0)\n",
        "\n",
        "    X_scaled = (X-X_min)/(X_max-X_min)\n",
        "\n",
        "    return X_scaled\n",
        "\n",
        "# Feature normalization\n",
        "X = min_max_scaler(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "print(\"Shape of X_train: \",X_train.shape, \"Shape of y_train: \",y_train.shape)\n",
        "print(\"Shape of X_test: \",X_test.shape, \"Shape of y_test: \",y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8SbmjD945Ra"
      },
      "source": [
        "### 2.2. Creating the Sigmoid Function\n",
        "Recall that for logistic regression, the model is represented as\n",
        "\n",
        "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
        "where function $g$ is the sigmoid function. The sigmoid function is defined as:\n",
        "\n",
        "$$g(z) = \\frac{1}{1+e^{-z}}$$\n",
        "\n",
        "\n",
        "The function below returns the value of the sigmoid function for an input numpy array z. If the numpy array 'z' stores multiple numbers, we'd like to apply the sigmoid function to each value in the input array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 590,
      "metadata": {
        "id": "G44B1vITbBpE"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Args:\n",
        "        z: A scalar or numpy array of any size.\n",
        "\n",
        "    Returns:\n",
        "        g: sigmoid(z)\n",
        "    \"\"\"\n",
        "    g = None\n",
        "    z = z.astype(float)\n",
        "    ### START CODE HERE ###\n",
        "    g = 1/(1+np.exp(-z))\n",
        "\n",
        "\n",
        "    ### END SOLUTION ###\n",
        "\n",
        "    return g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV-Ygf1R8HMl"
      },
      "source": [
        "### 2.3. Computing the loss Function\n",
        "\n",
        "Recall that for logistic regression, the cost function is of the form\n",
        "\n",
        "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
        "\n",
        "where\n",
        "* m is the number of training examples in the dataset\n",
        "\n",
        "\n",
        "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is -\n",
        "\n",
        "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
        "    \n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
        "\n",
        "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
        "\n",
        "Please complete the function loss_function that takes\n",
        "\n",
        " **X**  (input features)\n",
        "\n",
        " **y**  (class labels)\n",
        "\n",
        " **w**  (Parameters of the logistic regression model, (excluding the bias), a numpy array of the shape(1, number of features))\n",
        "\n",
        " **b**  (Bias value of the logistic regression model)\n",
        "\n",
        " You can use the Sigmoid function that you implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 591,
      "metadata": {
        "id": "h9RVguhHbEVk"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def loss_function(X, y, w, b):\n",
        " \"\"\"\n",
        " Computes the loss function for all the training examples\n",
        " Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b: Bias parameter (scalar) of the logistic regression model\n",
        "\n",
        "  Returns:\n",
        "        total_cost: The loss function value of using w and b as the parameters to fit the data points in X and y\n",
        "\n",
        " \"\"\"\n",
        " m, n = X.shape\n",
        "\n",
        " ### START CODE HERE ###\n",
        " z = np.dot(X, w.T) + b\n",
        " fx = sigmoid(z)\n",
        " cost = (-y * np.log(fx).T - (1 - y) * np.log(1 - fx).T)  \n",
        " total_cost = np.sum(cost) / m\n",
        " ### END CODE HERE ###\n",
        "\n",
        " return total_cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9YmUPnsASYE"
      },
      "source": [
        "### 2.4. Computing the Gradient of the Loss\n",
        "\n",
        "Recall that the gradient descent algorithm is:\n",
        "\n",
        "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
        "\n",
        "where, parameters $b$, $w_j$ are all updated simultaniously\n",
        "\n",
        "In this step, you are required to complete the `compute_gradient_logistic_regression` function to compute $\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$, $\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$ from equations (2) and (3) below.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
        "$$\n",
        "$$\n",
        "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
        "$$\n",
        "* m is the number of training examples in the dataset\n",
        "\n",
        "    \n",
        "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label\n",
        "\n",
        "You can use the sigmoid function that you implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 592,
      "metadata": {
        "id": "PWxbewKkbIB8"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def compute_gradient_logistic_regression(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the gradient values of the loss function\n",
        "    Args:\n",
        "       X: Input data of the shape (# of training samples, # of input features)\n",
        "       y: Target variable of the shape (# of training sample,)\n",
        "       w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "       b: Bias parameter of the logistic regression model of the shape (1,1) or a scaler\n",
        "    Returns:\n",
        "       dL_dw : The gradient of the cost w.r.t. the parameters w with shape same as w\n",
        "       dL_db : The gradient of the cost w.r.t. the parameter b with shape same as b\n",
        "    \"\"\"\n",
        "    m, n = X.shape\n",
        "    dj_dw = np.zeros(w.shape)\n",
        "\n",
        "    dj_db = 0.0  \n",
        "   \n",
        "    ### START CODE HERE ###\n",
        "    z = np.dot(X, w.T) + b\n",
        "    \n",
        "    fx = sigmoid(z)\n",
        "    loss = fx - y.reshape(-1, 1)\n",
        "\n",
        "    dj_dw = np.dot(loss.T, X) / m\n",
        "    dj_db = np.sum(loss) / m\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "\n",
        "    return dj_db, dj_dw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZMYX655MRFX"
      },
      "source": [
        "### 2.5. Training the model using Batch Gradient Descent\n",
        "\n",
        "Please complete the batch gradient descent algorithm for logistic regression to train and learn the parameters of the logistic regression model. You have to use ```loss_function``` and ```compute_gradient_logistic regression``` functions that you have implemented earlier in this assignment.\n",
        "\n",
        "In this ```batch_gradient_descent_logistic_regression``` function, you have to compute the gradient for the training samples and update the parameters $w$ and $b$ in every iteration:\n",
        "\n",
        "+ $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$\n",
        "\n",
        "+ $b \\leftarrow b - \\alpha \\frac{\\partial L}{\\partial b}$\n",
        "\n",
        "This function takes   ```X```    (input features),  ```y```  (class labels),  ```w_in```  (intial values of parameters(excluding bias)),  ```b_in```  (initial value for bias),  ```num_iters```   (number of iterations of training) as input.\n",
        "\n",
        "Additionally, you have compute the loss function values in every iteration and store it in the list variable ```loss_hist``` and print the loss value after every 100 iterations during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 593,
      "metadata": {
        "id": "JOi07RVNbL_7"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "def batch_gradient_descent_logistic_regression(X, y, w_in, b_in, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Batch gradient descent to learn the parameters (w and b) of the linear regression model and to print loss values\n",
        "    every 100 iterations\n",
        "\n",
        "    Args:\n",
        "        X: Input data of the shape (# of training samples, # of input features)\n",
        "        y: Target variable of the shape (# of training sample,)\n",
        "        w_in: Initial parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b_in: Initial bias parameter (scalar) of the logistic regression model\n",
        "        alpha: Learning rate\n",
        "        num_iters: number of iterations\n",
        "    Returns\n",
        "        w: Updated values of parameters of the model after training\n",
        "        b: Updated bias of the model after training\n",
        "        loss_hist: List of loss values for every iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # number of training examples\n",
        "    m = len(X)\n",
        "\n",
        "    # list to store the loss values for every iterations\n",
        "    loss_hist = []\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    for i in range(num_iters):\n",
        "        db,dw = compute_gradient_logistic_regression(X,y,w_in,b_in)\n",
        "        \n",
        "        dw = dw.astype(np.float64)\n",
        "        db = np.float64(db)\n",
        "\n",
        "        w_in -= dw*alpha\n",
        "        b_in -= db*alpha\n",
        "\n",
        "        loss  = loss_function(X,y,w_in,b_in)\n",
        "        loss_hist.append(loss)\n",
        "\n",
        "        if i%100 == 0:\n",
        "            print(f\"Iteration {i}, loss = {loss}\")\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return w_in, b_in, loss_hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yeLdzT9-5Wc"
      },
      "source": [
        "Now you have to intialize the model parameters ($w$ and $b$) and learning rate (```alpha```). The learning rate ```alpha``` has to be randomly initialized between 0.01 and 0.09. To randomly initialize the learning rate, you have to first set the last two digits of your roll number as the random seed using ```random.seed()``` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 594,
      "metadata": {
        "id": "MIBltyJh-Tjk"
      },
      "outputs": [],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "## set the last two digits of your roll number as the random seed\n",
        "random_seed = None\n",
        "### START CODE HERE ###\n",
        "random_seed = 32\n",
        "### END CODE HERE ###\n",
        "np.random.seed(random_seed)\n",
        "\n",
        "def initialize_parameters():\n",
        "    \"\"\"\n",
        "    This function randomly initializes the model parameters (w and b) and the hyperparameter alpha\n",
        "    Initial w and b should be randomly sampled from a normal distribution with mean 0\n",
        "    alpha should be randomly initialized between 0.01 and 0.09 by using last two digits of your roll number as the random seed\n",
        "    Args:\n",
        "        None\n",
        "    Returns:\n",
        "        initial_w: Initial parameters of the linear regression model (excluding the bias) of the shape (1, number of features)\n",
        "        initial_b: Initial bias parameter (scalar) of the linear regression model\n",
        "        alpha: Learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    initial_w = None\n",
        "    initial_b = None\n",
        "    alpha = None\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    np.random.seed(seed=random_seed)\n",
        "    initial_w = np.random.normal(0,1,size=(1,X_train.shape[1]))\n",
        "    initial_b = np.random.normal(0,1)\n",
        "    alpha= np.random.uniform(0.01,0.09)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return initial_w,initial_b,alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT5CbQ8dZS_J"
      },
      "source": [
        "The following cell runs the batch gradient algorithm for\n",
        "```num_iterations=1000``` to train the logistic regression model. You can change the number of iterations to check any improvements in the performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 595,
      "metadata": {
        "id": "KFTZi7pkbOdp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0, loss = 1.0384781501363591\n",
            "Iteration 100, loss = 0.7552436992212479\n",
            "Iteration 200, loss = 0.6958586532250082\n",
            "Iteration 300, loss = 0.6752743412651552\n",
            "Iteration 400, loss = 0.6598783030504529\n",
            "Iteration 500, loss = 0.6456560058068507\n",
            "Iteration 600, loss = 0.6321345728310367\n",
            "Iteration 700, loss = 0.6192330678441007\n",
            "Iteration 800, loss = 0.6069144867344325\n",
            "Iteration 900, loss = 0.595147806814082\n",
            "optimized parameter values w: [[-1.16941776 -0.15674895 -0.60114392 -0.11923267  0.30687715]]\n",
            "optimized parameter value b: 0.8430901374005475\n"
          ]
        }
      ],
      "source": [
        "# initialize the parameters (w an b) randomly\n",
        "initial_w, initial_b, alpha = initialize_parameters()\n",
        "num_iterations = 1000\n",
        "\n",
        "w, b, loss_hist = batch_gradient_descent_logistic_regression(X_train ,y_train, initial_w, initial_b, alpha, num_iterations)\n",
        "print(\"optimized parameter values w:\", w)\n",
        "print(\"optimized parameter value b:\", b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYP0-YUyY50x"
      },
      "source": [
        "### 2.6. Final Train and Test Accuracy\n",
        "After the logistic regression model is trained, we will predict the class labels for the training set and test set and we will compute the accuracy.\n",
        "\n",
        "Please complete the `predict` function to produce `1` or `0` predictions given a dataset and a learned parameter vector $w$ and $b$.\n",
        "- First you need to compute the prediction from the model $f(x^{(i)}) = g(w \\cdot x^{(i)})$ for every example\n",
        "\n",
        "- We interpret the output of the model ($f(x^{(i)})$) as the probability that $y^{(i)}=1$ given $x^{(i)}$ and parameterized by $w$.\n",
        "- Therefore, to get a final prediction ($y^{(i)}=0$ or $y^{(i)}=1$) from the logistic regression model, you can use the following heuristic -\n",
        "\n",
        "  if $f(x^{(i)}) >= 0.5$, predict $y^{(i)}=1$\n",
        "  \n",
        "  if $f(x^{(i)}) < 0.5$, predict $y^{(i)}=0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 596,
      "metadata": {
        "id": "mZDUV2iwbQ_R"
      },
      "outputs": [],
      "source": [
        "def predict(X, w, b):\n",
        "      \"\"\"\n",
        "      Predict whether the label is 0 or 1 using learned logistic regression parameters (w,b)\n",
        "\n",
        "      Args:\n",
        "        X: Input data of shape (number of sample, number of features)\n",
        "        w: Parameters of the logistic regression model (excluding the bias) of the shape (1, number of features)\n",
        "        b: Bias parameter of the logistic regression model\n",
        "\n",
        "      Returns:\n",
        "        p: Predictions for X using a threshold at 0.5\n",
        "      \"\"\"\n",
        "      m, n = X.shape\n",
        "      p = np.zeros(m)\n",
        "      ### START CODE HERE###\n",
        "      \n",
        "      z = np.dot(X,w.T)+b\n",
        "      y_predict = sigmoid(z)\n",
        "      \n",
        "      for i in range(m):\n",
        "          if y_predict[i]>=0.5:\n",
        "              p[i] = 1\n",
        "          else:\n",
        "              p[i]\n",
        "      ### END CODE HERE ###\n",
        "      return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsrATkIy2KVy"
      },
      "source": [
        "Now let's use this to compute the accuracy on the training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 597,
      "metadata": {
        "id": "UMmh6WdFbU9Y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Accuracy: 88.869443\n",
            "Test Accuracy: 87.513116\n"
          ]
        }
      ],
      "source": [
        "p_train = predict(X_train, w,b)\n",
        "print('Train Accuracy: %f'%(np.mean(p_train == y_train) * 100))\n",
        "p_test = predict(X_test, w,b)\n",
        "print('Test Accuracy: %f'%(np.mean(p_test == y_test) * 100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0VqjFy9ZK7D"
      },
      "source": [
        "Now, we plot the loss function values for every iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 598,
      "metadata": {
        "id": "fkmP3XHMTW74"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# PLotting the loss values for every training iterations\n",
        "\n",
        "loss_plot = [loss_hist[i] for i in range(len(loss_hist))]\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss function\")\n",
        "plt.plot(loss_plot)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgKAU9RBEUlL"
      },
      "source": [
        "### 2.7 Experimenting with different values of the Hyperparemeters\n",
        "\n",
        "Previously, we randomly sampled the learning rate and the number of features to train the model. Now, you have to manually choose the number of features and the learning rate. Then, you have to train the model again on the manually choosen hyperparameters (number of features and learning rate). In the next cell, you have to manually choose the hyperparameters and write the code to train the model.\n",
        "\n",
        "After the model is trained, you have to compare the performance of the model with random chosen hyperparameters and the model **with** manually chosen hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 599,
      "metadata": {
        "id": "xMcc3cuvIbD_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0, loss = 0.5482247772442365\n",
            "Iteration 100, loss = 0.32297338229395933\n",
            "Iteration 200, loss = 0.26746231935894726\n",
            "Iteration 300, loss = 0.24071746548478784\n",
            "Iteration 400, loss = 0.2250036077790857\n",
            "Iteration 500, loss = 0.21474586476053878\n",
            "Iteration 600, loss = 0.20759316092947108\n",
            "Iteration 700, loss = 0.20237305783548312\n",
            "Iteration 800, loss = 0.1984334133264805\n",
            "Iteration 900, loss = 0.19538238457290946\n",
            "alpha: 1\n",
            "Train Accuracy: 92.824641\n",
            "Test Accuracy: 91.815320\n",
            "Iteration 0, loss = 1.3287691895561224\n",
            "Iteration 100, loss = 0.2081337072086285\n",
            "Iteration 200, loss = 0.19816859393530348\n",
            "Iteration 300, loss = 0.1926136722163466\n",
            "Iteration 400, loss = 0.18914434622656584\n",
            "Iteration 500, loss = 0.18682299753097525\n",
            "Iteration 600, loss = 0.18519697211163624\n",
            "Iteration 700, loss = 0.1840202994077364\n",
            "Iteration 800, loss = 0.18314781521437068\n",
            "Iteration 900, loss = 0.18248852492054338\n",
            "alpha: 2\n",
            "Train Accuracy: 93.244662\n",
            "Test Accuracy: 92.025184\n",
            "Iteration 0, loss = 0.790919176776248\n",
            "Iteration 100, loss = 0.19076072662450205\n",
            "Iteration 200, loss = 0.18648977435899308\n",
            "Iteration 300, loss = 0.1842555219249627\n",
            "Iteration 400, loss = 0.1829209770739722\n",
            "Iteration 500, loss = 0.18206281601472016\n",
            "Iteration 600, loss = 0.18148546509379113\n",
            "Iteration 700, loss = 0.18108507069882174\n",
            "Iteration 800, loss = 0.18080121945388475\n",
            "Iteration 900, loss = 0.18059653223285102\n",
            "alpha: 3\n",
            "Train Accuracy: 93.174659\n",
            "Test Accuracy: 91.920252\n",
            "Iteration 0, loss = 0.31716622353167356\n",
            "Iteration 100, loss = 0.18496813991405298\n",
            "Iteration 200, loss = 0.18281192415867037\n",
            "Iteration 300, loss = 0.18174987704966022\n",
            "Iteration 400, loss = 0.1811430380036778\n",
            "Iteration 500, loss = 0.18076931027770185\n",
            "Iteration 600, loss = 0.18052895257561505\n",
            "Iteration 700, loss = 0.1803697952651005\n",
            "Iteration 800, loss = 0.18026198522363635\n",
            "Iteration 900, loss = 0.18018747071265168\n",
            "alpha: 4\n",
            "Train Accuracy: 93.139657\n",
            "Test Accuracy: 91.920252\n",
            "Iteration 0, loss = 0.18961136107464932\n",
            "Iteration 100, loss = 0.18248342980775326\n",
            "Iteration 200, loss = 0.18133476337323878\n",
            "Iteration 300, loss = 0.1807968384296356\n",
            "Iteration 400, loss = 0.18050148870906632\n",
            "Iteration 500, loss = 0.1803267751803453\n",
            "Iteration 600, loss = 0.18021869988703065\n",
            "Iteration 700, loss = 0.18014943353793178\n",
            "Iteration 800, loss = 0.18010346027202467\n",
            "Iteration 900, loss = 0.1800717409728723\n",
            "alpha: 5\n",
            "Train Accuracy: 93.104655\n",
            "Test Accuracy: 92.130115\n",
            "Iteration 0, loss = 0.19400624731024202\n",
            "Iteration 100, loss = 0.1813019492978581\n",
            "Iteration 200, loss = 0.18066690641164745\n",
            "Iteration 300, loss = 0.18038117016504404\n",
            "Iteration 400, loss = 0.18022926430300953\n",
            "Iteration 500, loss = 0.18014182879589444\n",
            "Iteration 600, loss = 0.18008846489211733\n",
            "Iteration 700, loss = 0.18005388602261196\n",
            "Iteration 800, loss = 0.18002990827629883\n",
            "Iteration 900, loss = 0.18001200510679105\n",
            "alpha: 6\n",
            "Train Accuracy: 93.139657\n",
            "Test Accuracy: 92.235047\n",
            "Iteration 0, loss = 0.21378709462036902\n",
            "Iteration 100, loss = 0.1806898282581967\n",
            "Iteration 200, loss = 0.18032437860660788\n",
            "Iteration 300, loss = 0.18016417392840595\n",
            "Iteration 400, loss = 0.18008033065243542\n",
            "Iteration 500, loss = 0.1800318892735184\n",
            "Iteration 600, loss = 0.18000117880644845\n",
            "Iteration 700, loss = 0.17997964874686231\n",
            "Iteration 800, loss = 0.1799629635234387\n",
            "Iteration 900, loss = 0.17994887249515581\n",
            "alpha: 7\n",
            "Train Accuracy: 93.139657\n",
            "Test Accuracy: 92.235047\n",
            "Iteration 0, loss = 0.2252834644148977\n",
            "Iteration 100, loss = 0.18033217464985363\n",
            "Iteration 200, loss = 0.18011224747586513\n",
            "Iteration 300, loss = 0.18001655994388305\n",
            "Iteration 400, loss = 0.1799657859720686\n",
            "Iteration 500, loss = 0.17993488404587027\n",
            "Iteration 600, loss = 0.17991334384040303\n",
            "Iteration 700, loss = 0.17989633742420852\n",
            "Iteration 800, loss = 0.17988157043108782\n",
            "Iteration 900, loss = 0.1798679337445141\n",
            "alpha: 8\n",
            "Train Accuracy: 93.139657\n",
            "Test Accuracy: 92.235047\n",
            "Iteration 0, loss = 0.2304292319658953\n",
            "Iteration 100, loss = 0.180088631154713\n",
            "Iteration 200, loss = 0.17994956748952598\n",
            "Iteration 300, loss = 0.1798880036680255\n",
            "Iteration 400, loss = 0.17985355105887238\n",
            "Iteration 500, loss = 0.1798304575182877\n",
            "Iteration 600, loss = 0.1798123770497205\n",
            "Iteration 700, loss = 0.1797965716484876\n",
            "Iteration 800, loss = 0.17978183041102458\n",
            "Iteration 900, loss = 0.17976761081627127\n",
            "alpha: 9\n",
            "Train Accuracy: 93.139657\n",
            "Test Accuracy: 92.235047\n",
            "Iteration 0, loss = 0.23276753665784478\n",
            "Iteration 100, loss = 0.17989468969484074\n",
            "Iteration 200, loss = 0.17980219064983868\n",
            "Iteration 300, loss = 0.1797592176959532\n",
            "Iteration 400, loss = 0.179732906581837\n",
            "Iteration 500, loss = 0.17971317016744948\n",
            "Iteration 600, loss = 0.17969615381618445\n",
            "Iteration 700, loss = 0.17968030694433887\n",
            "Iteration 800, loss = 0.17966499409888992\n",
            "Iteration 900, loss = 0.1796499519715466\n",
            "alpha: 10\n",
            "Train Accuracy: 93.139657\n",
            "Test Accuracy: 92.235047\n",
            "Iteration 0, loss = 0.23430641115839088\n",
            "Iteration 100, loss = 0.17971888909870698\n",
            "Iteration 200, loss = 0.17965546663290263\n",
            "Iteration 300, loss = 0.17962331244654664\n",
            "Iteration 400, loss = 0.17960121095976309\n",
            "Iteration 500, loss = 0.17958284066427016\n",
            "Iteration 600, loss = 0.17956592988617573\n",
            "Iteration 700, loss = 0.17954963032887006\n",
            "Iteration 800, loss = 0.1795336204165426\n",
            "Iteration 900, loss = 0.17951777677645428\n",
            "alpha: 11\n",
            "Train Accuracy: 93.104655\n",
            "Test Accuracy: 92.235047\n",
            "Iteration 0, loss = 0.2361500196621023\n",
            "Iteration 100, loss = 0.17953654833128393\n",
            "Iteration 200, loss = 0.17950051104610734\n",
            "Iteration 300, loss = 0.17947751875060716\n",
            "Iteration 400, loss = 0.1794584748995762\n",
            "Iteration 500, loss = 0.1794408436691537\n",
            "Iteration 600, loss = 0.17942376825824888\n",
            "Iteration 700, loss = 0.17940695092378947\n",
            "Iteration 800, loss = 0.17939028657928718\n",
            "Iteration 900, loss = 0.17937373722243907\n",
            "alpha: 12\n",
            "Train Accuracy: 93.069653\n",
            "Test Accuracy: 92.235047\n",
            "Iteration 0, loss = 0.23896906298657156\n",
            "Iteration 100, loss = 0.18166807359223996\n",
            "Iteration 200, loss = 0.18014906342461004\n",
            "Iteration 300, loss = 0.17962189305620252\n",
            "Iteration 400, loss = 0.1794114245362449\n",
            "Iteration 500, loss = 0.17932465001732903\n",
            "Iteration 600, loss = 0.1792833708169023\n",
            "Iteration 700, loss = 0.17925803409162058\n",
            "Iteration 800, loss = 0.17923814977751645\n",
            "Iteration 900, loss = 0.17922013763498706\n",
            "alpha: 13\n",
            "Train Accuracy: 93.034652\n",
            "Test Accuracy: 92.235047\n",
            "Iteration 0, loss = 0.24435081459253358\n",
            "Iteration 100, loss = 0.18336851425433823\n",
            "Iteration 200, loss = 0.18067211929920304\n",
            "Iteration 300, loss = 0.17970107297844065\n",
            "Iteration 400, loss = 0.17932707757011693\n",
            "Iteration 500, loss = 0.17918676524230623\n",
            "Iteration 600, loss = 0.17912931462508522\n",
            "Iteration 700, loss = 0.17910012128560995\n",
            "Iteration 800, loss = 0.18766427617463977\n",
            "Iteration 900, loss = 0.18161532243341022\n",
            "alpha: 14\n",
            "Train Accuracy: 93.034652\n",
            "Test Accuracy: 92.339979\n",
            "Iteration 0, loss = 0.4482393953145224\n",
            "Iteration 100, loss = 0.18567234336078756\n",
            "Iteration 200, loss = 0.18155983299958334\n",
            "Iteration 300, loss = 0.17991705496756105\n",
            "Iteration 400, loss = 0.17928874261161998\n",
            "Iteration 500, loss = 0.17925411811427797\n",
            "Iteration 600, loss = 0.1869154289810426\n",
            "Iteration 700, loss = 0.18184553497260333\n",
            "Iteration 800, loss = 0.17997206930558485\n",
            "Iteration 900, loss = 0.17924765719899152\n",
            "alpha: 15\n",
            "Train Accuracy: 93.069653\n",
            "Test Accuracy: 92.339979\n",
            "Iteration 0, loss = 0.2607164927562333\n",
            "Iteration 100, loss = 0.18802083670359113\n",
            "Iteration 200, loss = 0.1822582424246403\n",
            "Iteration 300, loss = 0.18001791023374808\n",
            "Iteration 400, loss = 0.1792049739719957\n",
            "Iteration 500, loss = 0.19013243543509886\n",
            "Iteration 600, loss = 0.1830378679626344\n",
            "Iteration 700, loss = 0.1802878852379353\n",
            "Iteration 800, loss = 0.1792264450985261\n",
            "Iteration 900, loss = 0.19500322617113758\n",
            "alpha: 16\n",
            "Train Accuracy: 92.999650\n",
            "Test Accuracy: 92.235047\n",
            "Iteration 0, loss = 1.0726587598242663\n",
            "Iteration 100, loss = 0.19156558506872334\n",
            "Iteration 200, loss = 0.18389183026812098\n",
            "Iteration 300, loss = 0.18050810527429664\n",
            "Iteration 400, loss = 0.21462354662583408\n",
            "Iteration 500, loss = 0.18893902933496595\n",
            "Iteration 600, loss = 0.18261874535852166\n",
            "Iteration 700, loss = 0.1799483340562189\n",
            "Iteration 800, loss = 0.20104501202898148\n",
            "Iteration 900, loss = 0.18646620011174986\n",
            "alpha: 17\n",
            "Train Accuracy: 93.034652\n",
            "Test Accuracy: 92.339979\n",
            "Iteration 0, loss = 0.5132355483468659\n",
            "Iteration 100, loss = 0.19475885152788538\n",
            "Iteration 200, loss = 0.1850471572056762\n",
            "Iteration 300, loss = 0.18075763543788625\n",
            "Iteration 400, loss = 0.2049950962969449\n",
            "Iteration 500, loss = 0.18834926427889712\n",
            "Iteration 600, loss = 0.18214519994646688\n",
            "Iteration 700, loss = 2.0795747389701864\n",
            "Iteration 800, loss = 0.1935252171461301\n",
            "Iteration 900, loss = 0.1843976401102478\n",
            "alpha: 18\n",
            "Train Accuracy: 93.034652\n",
            "Test Accuracy: 92.339979\n",
            "Iteration 0, loss = 0.36563612736152673\n",
            "Iteration 100, loss = 0.1981393469238811\n",
            "Iteration 200, loss = 0.1862764777792592\n",
            "Iteration 300, loss = 0.18100909656940942\n",
            "Iteration 400, loss = 0.20295616727734453\n",
            "Iteration 500, loss = 0.18828114994610723\n",
            "Iteration 600, loss = 0.1818353564819712\n",
            "Iteration 700, loss = 0.2119088658862941\n",
            "Iteration 800, loss = 0.19100422702155087\n",
            "Iteration 900, loss = 0.18303656960431708\n",
            "alpha: 19\n",
            "Train Accuracy: 93.279664\n",
            "Test Accuracy: 92.130115\n",
            "Iteration 0, loss = 2.016765523454432\n",
            "Iteration 100, loss = 0.19954811535034717\n",
            "Iteration 200, loss = 0.18670705352031194\n",
            "Iteration 300, loss = 1.4733063352830007\n",
            "Iteration 400, loss = 0.20053612213036642\n",
            "Iteration 500, loss = 0.18736019215507482\n",
            "Iteration 600, loss = 0.18845591138151352\n",
            "Iteration 700, loss = 0.20333256654052845\n",
            "Iteration 800, loss = 0.18857207790630703\n",
            "Iteration 900, loss = 0.1817027765878292\n",
            "alpha: 20\n",
            "Train Accuracy: 93.209660\n",
            "Test Accuracy: 91.815320\n",
            "Train Accuracy Before:  88.8694434721736 , Test Accuracy Before:  87.5131164742917 Alpha Before :  0.01808692482468998\n",
            "Maximum Train Accuracy:  93.03465173258662 , Maximum Test Accuracy:  92.33997901364114 Maximum Alpha:  14\n"
          ]
        }
      ],
      "source": [
        "## CODE REQUIRED ##\n",
        "\n",
        "\"\"\"\n",
        "Manually choose the hyperparameters (learning rate and number of features) and train the model.\n",
        "Then compare the performance with random chosen hyperparameters and manually chosen hyperparameters.\n",
        "\"\"\"\n",
        "\n",
        "### START CODE HERE ###\n",
        "num_iterations = 1000\n",
        "new_X, new_y = load_data(filepath)\n",
        "new_X = min_max_scaler(new_X)\n",
        "new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(new_X, new_y, test_size=0.25, random_state=42)\n",
        "new_initial_w = np.random.normal(0,1,size=(1,new_X_train.shape[1]))\n",
        "new_initial_b = np.random.normal(0,1)\n",
        "new_alpha=1\n",
        "\n",
        "max_train_accuracy = 0\n",
        "max_test_accuracy = 0\n",
        "max_alpha = 0\n",
        "\n",
        "while new_alpha<=20:\n",
        "  w, b, loss_hist = batch_gradient_descent_logistic_regression(new_X_train ,new_y_train, new_initial_w, new_initial_b, new_alpha, num_iterations)\n",
        "  print('alpha:',new_alpha)\n",
        "  new_p_train = predict(new_X_train, w,b)\n",
        "  print('Train Accuracy: %f'%(np.mean(new_p_train == new_y_train) * 100))\n",
        "  new_p_test = predict(new_X_test, w,b)\n",
        "  print('Test Accuracy: %f'%(np.mean(new_p_test == new_y_test) * 100))\n",
        "  if np.mean(new_p_test == y_test) > max_test_accuracy:\n",
        "    max_test_accuracy = np.mean(new_p_test == new_y_test)\n",
        "    max_train_accuracy = np.mean(new_p_train == new_y_train)\n",
        "    max_alpha = new_alpha\n",
        "  new_alpha=new_alpha+1\n",
        "\n",
        "print(\"Train Accuracy Before: \",np.mean(p_train == y_train)*100, \", Test Accuracy Before: \",np.mean(p_test == y_test)*100,\"Alpha Before : \",alpha)\n",
        "print(\"Maximum Train Accuracy: \",max_train_accuracy*100, \", Maximum Test Accuracy: \",max_test_accuracy*100,\"Maximum Alpha: \",max_alpha)\n",
        "### END CODE HERE ###"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
